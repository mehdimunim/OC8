{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30dac86e-8894-44da-a3b2-7b19edffa1ac",
   "metadata": {
    "id": "30dac86e-8894-44da-a3b2-7b19edffa1ac"
   },
   "source": [
    "# Notebook 3 : Modélisation\n",
    "\n",
    "Ce notebook a pour objectif de développer et comparer différents modèles de segmentation d'images pour identifier les objets présents dans des scènes de rue. Nous utiliserons le dataset Cityscapes, qui contient des images avec des annotations précises pour différents objets (voitures, piétons, bâtiments, etc.).\n",
    "\n",
    "Nous avons précédemment explorer les données (notebook 1), puis effectuer un prétraitement (notebook 2).\n",
    "\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "* Implémenter et comparer différents modèles de segmentation d'images (UNet, SegNet, etc.)\n",
    "* Tester différentes fonctions de perte :\n",
    "  - Entropie croisée (categorical cross-entropy)\n",
    "  - Dice loss\n",
    "  - Compromis entre les deux loss\n",
    "* Évaluer les performances des modèles avec différentes métriques\n",
    "    - Dice coefficient\n",
    "    - Intersection over Union (IoU)\n",
    "* Expérimenter avec l'augmentation de données pour améliorer la robustesse des modèles\n",
    "\n",
    "## Données\n",
    "\n",
    "Le dataset Cityscapes est utilisé pour ce projet. Il contient des images de scènes de rue avec des annotations pour 8 classes d'objets :\n",
    "- route (flat)\n",
    "- humain (human)\n",
    "- véhicule (vehicle)\n",
    "- bâtiment (construction)\n",
    "- objets (object)\n",
    "- nature (nature)\n",
    "- ciel (sky)\n",
    "- vide (void)\n",
    "  \n",
    "## Librairies\n",
    "\n",
    "* TensorFlow et Keras pour la construction et l'entraînement des modèles\n",
    "* MLflow pour le suivi des expériences\n",
    "* OpenCV pour le traitement des images\n",
    "* NumPy pour les opérations numériques\n",
    "* Matplotlib pour la visualisation des résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062db69-035d-47ae-b347-7571d2336c9e",
   "metadata": {
    "id": "5062db69-035d-47ae-b347-7571d2336c9e"
   },
   "source": [
    "## Partie 1 : Configuration de l'environnement et des paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-zWxkoRYwUGB",
   "metadata": {
    "id": "-zWxkoRYwUGB"
   },
   "source": [
    "### Installation de mflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "UbPOJepdhKCw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-21T12:08:02.953623Z",
     "iopub.status.busy": "2025-04-21T12:08:02.953174Z",
     "iopub.status.idle": "2025-04-21T12:08:07.589910Z",
     "shell.execute_reply": "2025-04-21T12:08:07.589141Z",
     "shell.execute_reply.started": "2025-04-21T12:08:02.953578Z"
    },
    "executionInfo": {
     "elapsed": 14422,
     "status": "ok",
     "timestamp": 1745230623183,
     "user": {
      "displayName": "Mehdi MUNIM",
      "userId": "14326957258997738516"
     },
     "user_tz": -120
    },
    "id": "UbPOJepdhKCw",
    "outputId": "42f2c7ff-1558-42f1-9c2e-a20c3e45f57e"
   },
   "outputs": [],
   "source": [
    "!pip install mlflow -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3S7wV3gDwVzZ",
   "metadata": {
    "id": "3S7wV3gDwVzZ"
   },
   "source": [
    "### Imports et configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "030ea610-f215-4910-b231-a82ce5297817",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-21T12:08:07.591445Z",
     "iopub.status.busy": "2025-04-21T12:08:07.590959Z",
     "iopub.status.idle": "2025-04-21T12:08:09.500456Z",
     "shell.execute_reply": "2025-04-21T12:08:09.494961Z",
     "shell.execute_reply.started": "2025-04-21T12:08:07.591390Z"
    },
    "executionInfo": {
     "elapsed": 12935,
     "status": "ok",
     "timestamp": 1745230636121,
     "user": {
      "displayName": "Mehdi MUNIM",
      "userId": "14326957258997738516"
     },
     "user_tz": -120
    },
    "id": "030ea610-f215-4910-b231-a82ce5297817",
    "outputId": "dd158683-4865-456b-e83b-b77df8037991"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Image processing\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Data manipulation and visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Image processing\n",
    "import cv2\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# MLflow\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "\n",
    "\n",
    "# Chemins vers les données (train / test / val)\n",
    "DATA_DIR = '../data/processed'\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(DATA_DIR, \"val\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test\")\n",
    "\n",
    "# Paramètres\n",
    "## données\n",
    "SAMPLING = 0.75\n",
    "NUM_CLASSES = 8\n",
    "## Tailles des images redimensionnées (identique au notebook 2)\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 256\n",
    "## batch_size / nombre d'epochs\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "\n",
    "# Configuration de MLflow\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "mlflow.set_experiment(\"segmentation_images_cityscapes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfeb80f-051a-42e5-be7c-3354da82ea41",
   "metadata": {
    "id": "2dfeb80f-051a-42e5-be7c-3354da82ea41"
   },
   "source": [
    "## Partie 2 : Chargement et préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pNcyyPW5wjAN",
   "metadata": {
    "id": "pNcyyPW5wjAN"
   },
   "source": [
    "### Chargement et préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebf3ca0-e449-4c37-978f-78edb9be9a95",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T12:08:09.502045Z",
     "iopub.status.idle": "2025-04-21T12:08:09.502703Z",
     "shell.execute_reply": "2025-04-21T12:08:09.502397Z",
     "shell.execute_reply.started": "2025-04-21T12:08:09.502370Z"
    },
    "executionInfo": {
     "elapsed": 18097,
     "status": "ok",
     "timestamp": 1745230654211,
     "user": {
      "displayName": "Mehdi MUNIM",
      "userId": "14326957258997738516"
     },
     "user_tz": -120
    },
    "id": "bebf3ca0-e449-4c37-978f-78edb9be9a95"
   },
   "outputs": [],
   "source": [
    "# Chargement et préparation des données\n",
    "\n",
    "def load_data(data_dir, sample_ratio = 0.1):\n",
    "    images = sorted(glob.glob(os.path.join(data_dir, \"*\", \"*_image.png\")))\n",
    "    masks = sorted(glob.glob(os.path.join(data_dir, \"*\", \"*_mask.png\")))\n",
    "\n",
    "    # S'assurer que le nombre d'images et de masques correspond\n",
    "    assert len(images) == len(masks), \"Le nombre d'images et de masques ne correspond pas\"\n",
    "\n",
    "    # Calculer le nombre d'échantillons à sélectionner\n",
    "    sample_size = int(len(images) * sample_ratio)\n",
    "\n",
    "    # Créer des paires d'images et de masques\n",
    "    paired_data = list(zip(images, masks))\n",
    "\n",
    "    # Mélanger aléatoirement les paires\n",
    "    random.shuffle(paired_data)\n",
    "\n",
    "    # Sélectionner un sous-ensemble\n",
    "    sampled_data = paired_data[:sample_size]\n",
    "\n",
    "    # Séparer les images et les masques échantillonnés\n",
    "    sampled_images, sampled_masks = zip(*sampled_data)\n",
    "\n",
    "    return list(sampled_images), list(sampled_masks)\n",
    "    return images, masks\n",
    "\n",
    "\n",
    "\n",
    "## chargement des données preprocéssées\n",
    "train_images, train_masks = load_data(TRAIN_DIR, sample_ratio = SAMPLING)\n",
    "val_images, val_masks = load_data(VAL_DIR, sample_ratio = SAMPLING)\n",
    "test_images, test_masks = load_data(TEST_DIR, sample_ratio = SAMPLING)\n",
    "\n",
    "def data_generator(images, masks, batch_size, num_classes):\n",
    "    \"\"\"\n",
    "    Transforme un générateur de données (images et masques) en un tf.data.Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def generator():\n",
    "        \"\"\"Générateur pour le tf.data.Dataset.\"\"\"\n",
    "        for i in range(0, len(images)):\n",
    "            # Chargement de l'image\n",
    "            image = cv2.imread(images[i])\n",
    "            image = image / 255.0\n",
    "\n",
    "            # Chargement du masque en grayscale\n",
    "            mask = cv2.imread(masks[i], cv2.IMREAD_GRAYSCALE)\n",
    "            mask = keras.utils.to_categorical(mask, num_classes=num_classes)\n",
    "\n",
    "            yield image, mask\n",
    "\n",
    "    # Définir les types de données de sortie\n",
    "    output_types = (tf.float32, tf.float32)\n",
    "\n",
    "    # Définir les formes des données de sortie (None pour les dimensions variables)\n",
    "    # Ici, on suppose que toutes les images ont la même taille\n",
    "    image_shape = cv2.imread(images[0]).shape\n",
    "    mask_shape = (image_shape[0], image_shape[1], num_classes)\n",
    "\n",
    "    output_shapes = (tf.TensorShape(image_shape), tf.TensorShape(mask_shape))\n",
    "\n",
    "    # Créer le tf.data.Dataset à partir du générateur\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_types=output_types,\n",
    "        output_shapes=output_shapes\n",
    "    )\n",
    "\n",
    "    # Batching\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5U7wtnj3wlpJ",
   "metadata": {
    "id": "5U7wtnj3wlpJ"
   },
   "source": [
    "### Résumé d'informations sur le dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vpBXqDoaiGZ_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2025-04-21T12:08:09.506342Z",
     "iopub.status.idle": "2025-04-21T12:08:09.506998Z",
     "shell.execute_reply": "2025-04-21T12:08:09.506697Z",
     "shell.execute_reply.started": "2025-04-21T12:08:09.506669Z"
    },
    "executionInfo": {
     "elapsed": 17513,
     "status": "ok",
     "timestamp": 1745230671732,
     "user": {
      "displayName": "Mehdi MUNIM",
      "userId": "14326957258997738516"
     },
     "user_tz": -120
    },
    "id": "vpBXqDoaiGZ_",
    "outputId": "d230ac8b-1b21-4e61-c8a1-a9f3d2220058"
   },
   "outputs": [],
   "source": [
    "\n",
    "def print_dataset_info(images, masks, dataset_name):\n",
    "    \"\"\"Affiche les informations d'un dataset\"\"\"\n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"Informations du dataset {dataset_name}:\")\n",
    "    print(f\"{'='*40}\")\n",
    "\n",
    "    # Nombre d'échantillons\n",
    "    print(f\"Nombre d'images: {len(images)}\")\n",
    "    print(f\"Nombre de masques: {len(masks)}\")\n",
    "\n",
    "    # Dimensions de la première image\n",
    "    sample_image = cv2.imread(images[0])\n",
    "    print(f\"\\nDimensions image (H, W, C): {sample_image.shape}\")\n",
    "\n",
    "    # Dimensions du premier masque\n",
    "    sample_mask = cv2.imread(masks[0], cv2.IMREAD_GRAYSCALE)\n",
    "    print(f\"Dimensions masque (H, W): {sample_mask.shape}\")\n",
    "\n",
    "    # Vérification de la cohérence des dimensions\n",
    "    for img, mask in zip(images[:3], masks[:3]):  # Vérification sur 3 échantillons\n",
    "        img_shape = cv2.imread(img).shape[:2]\n",
    "        mask_shape = cv2.imread(mask, cv2.IMREAD_GRAYSCALE).shape\n",
    "        if img_shape != mask_shape:\n",
    "            print(f\"Attention ! Incohérence de dimensions entre {img} et {mask}\")\n",
    "\n",
    "# Affichage des informations pour chaque dataset\n",
    "print_dataset_info(train_images, train_masks, \"TRAIN\")\n",
    "print_dataset_info(val_images, val_masks, \"VALIDATION\")\n",
    "print_dataset_info(test_images, test_masks, \"TEST\")\n",
    "\n",
    "# Information sur le nombre de classes\n",
    "print(f\"\\n{'='*40}\")\n",
    "print(f\"Nombre de classes: {NUM_CLASSES}\")\n",
    "print(f\"Taille du batch: {BATCH_SIZE}\")\n",
    "print(f\"{'='*40}\")\n",
    "\n",
    "# Vérification supplémentaire des canaux\n",
    "sample_image = cv2.imread(train_images[0])\n",
    "sample_mask = cv2.imread(train_masks[0], cv2.IMREAD_GRAYSCALE)\n",
    "print(f\"\\nExemple de valeurs de pixel (Image): Min={sample_image.min()} Max={sample_image.max()}\")\n",
    "print(f\"Exemple de valeurs de pixel (Masque): Classes uniques={np.unique(sample_mask)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c2678d-0fa3-4e8b-8b26-57a44ae85fa4",
   "metadata": {
    "id": "f4c2678d-0fa3-4e8b-8b26-57a44ae85fa4"
   },
   "source": [
    "## Partie 3 : Définition des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "txQWsXuOieEt",
   "metadata": {
    "id": "txQWsXuOieEt"
   },
   "source": [
    "Plusieurs modèles (deep learning) sont utilisés dans notre cas. Ils sont spécifiques à la segmentation sémantique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7443d9e-a575-4d8c-8403-0b04f519b63b",
   "metadata": {
    "id": "f7443d9e-a575-4d8c-8403-0b04f519b63b"
   },
   "source": [
    "### 1- UNet mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c21c9f0-42fa-43d8-9158-0a24aa9213cf",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T12:08:09.509354Z",
     "iopub.status.idle": "2025-04-21T12:08:09.510327Z",
     "shell.execute_reply": "2025-04-21T12:08:09.509978Z",
     "shell.execute_reply.started": "2025-04-21T12:08:09.509706Z"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1745230671758,
     "user": {
      "displayName": "Mehdi MUNIM",
      "userId": "14326957258997738516"
     },
     "user_tz": -120
    },
    "id": "3c21c9f0-42fa-43d8-9158-0a24aa9213cf"
   },
   "outputs": [],
   "source": [
    "def build_unet_mini(img_height, img_width, num_classes):\n",
    "    inputs = keras.layers.Input(shape=(img_height, img_width, 3))\n",
    "\n",
    "    # Downsampling\n",
    "    conv1 = keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    pool1 = keras.layers.MaxPooling2D((2, 2))(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    pool2 = keras.layers.MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "    # Bottleneck\n",
    "    conv3 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(pool2)\n",
    "\n",
    "    # Upsampling\n",
    "    up4 = keras.layers.UpSampling2D((2, 2))(conv3)\n",
    "    merge4 = keras.layers.concatenate([conv2, up4], axis=-1)\n",
    "    conv4 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(merge4)\n",
    "\n",
    "    up5 = keras.layers.UpSampling2D((2, 2))(conv4)\n",
    "    merge5 = keras.layers.concatenate([conv1, up5], axis=-1)\n",
    "    conv5 = keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(merge5)\n",
    "\n",
    "    # Output\n",
    "    outputs = keras.layers.Conv2D(num_classes, (1, 1), activation='softmax')(conv5)\n",
    "\n",
    "    model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728eb44f-ff9a-420e-a04d-edee09721230",
   "metadata": {
    "id": "728eb44f-ff9a-420e-a04d-edee09721230"
   },
   "source": [
    "### 2- Modèle UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468b1579-2013-4754-9904-436ac8d6e2e5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T12:08:09.512322Z",
     "iopub.status.idle": "2025-04-21T12:08:09.514875Z",
     "shell.execute_reply": "2025-04-21T12:08:09.513149Z",
     "shell.execute_reply.started": "2025-04-21T12:08:09.513112Z"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1745230671771,
     "user": {
      "displayName": "Mehdi MUNIM",
      "userId": "14326957258997738516"
     },
     "user_tz": -120
    },
    "id": "468b1579-2013-4754-9904-436ac8d6e2e5"
   },
   "outputs": [],
   "source": [
    "def build_unet(img_height, img_width, num_classes):\n",
    "    \"\"\"\n",
    "    Définition du modèle UNET spécifique à la segmentation d'images.\n",
    "    U-Net : https://fr.wikipedia.org/wiki/U-Net\n",
    "    \"\"\"\n",
    "\n",
    "    ## Entrée\n",
    "    inputs = keras.layers.Input(shape=(img_height, img_width, 3))\n",
    "\n",
    "    ## Bloc 1\n",
    "    conv1 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    pool1 = keras.layers.MaxPooling2D((2, 2))(conv1)\n",
    "\n",
    "    ## Bloc 2\n",
    "    conv2 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    pool2 = keras.layers.MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "    ## Bloc 3\n",
    "    conv3 = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n",
    "\n",
    "    ## Bloc 4\n",
    "    up4 = keras.layers.UpSampling2D((2, 2))(conv3)\n",
    "    merge4 = keras.layers.concatenate([conv2, up4], axis=-1)\n",
    "    conv4 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(merge4)\n",
    "\n",
    "    ## Bloc 5\n",
    "    up5 = keras.layers.UpSampling2D((2, 2))(conv4)\n",
    "    merge5 = keras.layers.concatenate([conv1, up5], axis=-1)\n",
    "    conv5 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(merge5)\n",
    "\n",
    "    ## Couche de sortie\n",
    "    outputs = keras.layers.Conv2D(num_classes, (1, 1), activation='softmax')(conv5)\n",
    "    model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6fd16d-e9f2-4cc0-813f-a65952c83823",
   "metadata": {
    "id": "aa6fd16d-e9f2-4cc0-813f-a65952c83823"
   },
   "source": [
    "### 3- Modèle VGG16/UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01001ed2-3cd8-41d9-87a4-2ae76eecde04",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T12:08:09.517986Z",
     "iopub.status.idle": "2025-04-21T12:08:09.518481Z",
     "shell.execute_reply": "2025-04-21T12:08:09.518288Z",
     "shell.execute_reply.started": "2025-04-21T12:08:09.518268Z"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1745230671781,
     "user": {
      "displayName": "Mehdi MUNIM",
      "userId": "14326957258997738516"
     },
     "user_tz": -120
    },
    "id": "01001ed2-3cd8-41d9-87a4-2ae76eecde04"
   },
   "outputs": [],
   "source": [
    "def build_vgg16_unet(img_height, img_width, num_classes):\n",
    "    \"\"\"\n",
    "    Construit un modèle VGG16-UNet avec des couches d'upsampling supplémentaires\n",
    "    pour s'assurer que la taille de la sortie correspond à la taille de l'entrée.\n",
    "    \"\"\"\n",
    "    # Charger le modèle VGG16 pré-entraîné sans la partie classification\n",
    "    vgg16 = keras.applications.VGG16(input_shape=(img_height, img_width, 3),\n",
    "                                     include_top=False,\n",
    "                                     weights='imagenet')\n",
    "\n",
    "    # Encoder (VGG16)\n",
    "    vgg16_output = vgg16.output\n",
    "\n",
    "    # Decoder (UNet-like)\n",
    "    # Ajouter des couches Conv2DTranspose supplémentaires pour augmenter la taille\n",
    "    up0 = keras.layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(vgg16_output) # Taille: H/16 * 2, W/16 * 2\n",
    "    merge0 = keras.layers.concatenate([vgg16.get_layer('block5_conv3').output, up0], axis=-1)\n",
    "    conv0 = keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(merge0)\n",
    "\n",
    "    up1 = keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv0) # Taille: H/8 * 2, W/8 * 2\n",
    "    merge1 = keras.layers.concatenate([vgg16.get_layer('block4_conv3').output, up1], axis=-1)\n",
    "    conv1 = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(merge1)\n",
    "\n",
    "    up2 = keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv1) # Taille: H/4 * 2, W/4 * 2\n",
    "    merge2 = keras.layers.concatenate([vgg16.get_layer('block3_conv3').output, up2], axis=-1)\n",
    "    conv2 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(merge2)\n",
    "\n",
    "    up3 = keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv2) # Taille: H/2 * 2, W/2 * 2\n",
    "    merge3 = keras.layers.concatenate([vgg16.get_layer('block2_conv2').output, up3], axis=-1)\n",
    "    conv3 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(merge3)\n",
    "\n",
    "    up4 = keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv3) # Taille: H * 2, W * 2\n",
    "    merge4 = keras.layers.concatenate([vgg16.get_layer('block1_conv2').output, up4], axis=-1)\n",
    "    conv4 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(merge4)\n",
    "\n",
    "    # Output\n",
    "    outputs = keras.layers.Conv2D(num_classes, (1, 1), activation='softmax')(conv4)\n",
    "\n",
    "    model = keras.models.Model(inputs=vgg16.input, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64b2096-50ce-4277-af23-f3a496746fc1",
   "metadata": {
    "id": "a64b2096-50ce-4277-af23-f3a496746fc1"
   },
   "source": [
    "### 4- MobileNetV2_Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6329a360-55b1-4a6b-968b-6892e72a5f2e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T12:08:09.523929Z",
     "iopub.status.idle": "2025-04-21T12:08:09.524460Z",
     "shell.execute_reply": "2025-04-21T12:08:09.524260Z",
     "shell.execute_reply.started": "2025-04-21T12:08:09.524241Z"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1745230671787,
     "user": {
      "displayName": "Mehdi MUNIM",
      "userId": "14326957258997738516"
     },
     "user_tz": -120
    },
    "id": "6329a360-55b1-4a6b-968b-6892e72a5f2e"
   },
   "outputs": [],
   "source": [
    "def build_mobilenetv2_unet(img_height, img_width, num_classes):\n",
    "    \"\"\"Construit un modèle MobileNetV2-UNet.\"\"\"\n",
    "    # Charger le modèle MobileNetV2 pré-entraîné sans la partie classification\n",
    "    mobilenetv2 = keras.applications.MobileNetV2(input_shape=(img_height, img_width, 3),\n",
    "                                     include_top=False,\n",
    "                                     weights='imagenet')\n",
    "\n",
    "    # Récupérer la sortie de MobileNetV2\n",
    "    mobilenet_output = mobilenetv2.output\n",
    "\n",
    "    # Reshape de la sortie de MobileNetV2\n",
    "    reshape = keras.layers.Reshape((img_height // 32, img_width // 32, mobilenet_output.shape[-1]))(mobilenet_output)\n",
    "\n",
    "    # Decoder (UNet-like) avec 5 couches Conv2DTranspose\n",
    "    up1 = keras.layers.Conv2DTranspose(512, (3, 3), strides=(2, 2), padding='same')(reshape) # 8x16\n",
    "    conv1 = keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(up1)\n",
    "\n",
    "    up2 = keras.layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same')(conv1) # 16x32\n",
    "    conv2 = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(up2)\n",
    "\n",
    "    up3 = keras.layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same')(conv2) # 32x64\n",
    "    conv3 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(up3)\n",
    "\n",
    "    up4 = keras.layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same')(conv3) # 64x128\n",
    "    conv4 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(up4)\n",
    "\n",
    "    up5 = keras.layers.Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same')(conv4) # 128x256\n",
    "    conv5 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(up5)\n",
    "\n",
    "    # Output\n",
    "    outputs = keras.layers.Conv2D(num_classes, (1, 1), activation='softmax')(conv5)\n",
    "\n",
    "    model = keras.models.Model(inputs=mobilenetv2.input, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e1e58-7efa-45d2-a06b-b162be7c245f",
   "metadata": {
    "id": "9c7e1e58-7efa-45d2-a06b-b162be7c245f"
   },
   "source": [
    "## Partie 4 : Définition des fonctions de perte et des métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5940ff-33b8-4d42-91a7-9c68abc19c46",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T12:08:09.526071Z",
     "iopub.status.idle": "2025-04-21T12:08:09.527143Z",
     "shell.execute_reply": "2025-04-21T12:08:09.526810Z",
     "shell.execute_reply.started": "2025-04-21T12:08:09.526778Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745230671794,
     "user": {
      "displayName": "Mehdi MUNIM",
      "userId": "14326957258997738516"
     },
     "user_tz": -120
    },
    "id": "3a5940ff-33b8-4d42-91a7-9c68abc19c46"
   },
   "outputs": [],
   "source": [
    "\n",
    "## (1) Métriques\n",
    "\n",
    "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Calcule le coefficient de Dice, une métrique de similarité entre deux ensembles.\n",
    "    Il est souvent utilisé pour évaluer la performance des modèles de segmentation.\n",
    "\n",
    "    Args:\n",
    "        y_true (Tensor): Les valeurs de vérité terrain (ground truth).\n",
    "        y_pred (Tensor): Les prédictions du modèle.\n",
    "        smooth (float, optional): Un terme de lissage pour éviter la division par zéro. Defaults to 1e-6.\n",
    "\n",
    "    Returns:\n",
    "        float: Le coefficient de Dice, une valeur entre 0 et 1 inclusivement.\n",
    "\n",
    "    Plage de valeur :\n",
    "        - 0 : Indique une absence totale de chevauchement entre les prédictions et la vérité terrain.\n",
    "        - 1 : Indique un chevauchement parfait entre les prédictions et la vérité terrain (les ensembles sont identiques).\n",
    "    \"\"\"\n",
    "    y_true_f = keras.backend.flatten(tf.cast(y_true, tf.float32)) # Conversion de y_true en float32\n",
    "    y_pred_f = keras.backend.flatten(y_pred)\n",
    "    intersection = keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (keras.backend.sum(y_true_f) + keras.backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "def iou_metric(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Calcule l'IoU (Intersection over Union), également appelé indice de Jaccard,\n",
    "    une métrique couramment utilisée pour évaluer la performance des modèles de segmentation.\n",
    "\n",
    "    Args:\n",
    "        y_true (Tensor): Les valeurs de vérité terrain (ground truth).\n",
    "        y_pred (Tensor): Les prédictions du modèle.\n",
    "        smooth (float, optional): Un terme de lissage pour éviter la division par zéro. Defaults to 1e-6.\n",
    "\n",
    "    Returns:\n",
    "        float: L'IoU, une valeur entre 0 et 1 inclusivement.\n",
    "\n",
    "    Plage de valeur :\n",
    "        - 0 : Indique une absence totale de chevauchement entre les prédictions et la vérité terrain.\n",
    "        - 1 : Indique un chevauchement parfait entre les prédictions et la vérité terrain.\n",
    "    \"\"\"\n",
    "    y_true_f = keras.backend.flatten(tf.cast(y_true, tf.float32)) # Conversion de y_true en float32\n",
    "    y_pred_f = keras.backend.flatten(y_pred)\n",
    "    intersection = keras.backend.sum(y_true_f * y_pred_f)\n",
    "    union = keras.backend.sum(y_true_f) + keras.backend.sum(y_pred_f) - intersection\n",
    "    return (intersection + smooth) / (union + smooth)\n",
    "\n",
    "\n",
    "## (2) Fonctions de perte (loss)\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "### on utilisera également la categorical crossentropy\n",
    "\n",
    "def mixed_loss(y_true, y_pred):\n",
    "    \"\"\"Fonction de perte combinant categorical crossentropy et dice loss.\"\"\"\n",
    "    return 0.5 * keras.losses.CategoricalCrossentropy()(y_true, y_pred) + 0.5 * dice_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ad58f-684e-4213-b4fd-252b62292506",
   "metadata": {
    "id": "e08ad58f-684e-4213-b4fd-252b62292506"
   },
   "source": [
    "## Partie 5 : Augmentation des données (Data Augmentation)\n",
    "\n",
    "On génère des données \"augmentées\", à savoir en faisant des modifications légères :\n",
    "\n",
    "- flip horizontal (retournement)\n",
    "\n",
    "- rotation\n",
    "\n",
    "- zoom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dd6459-6d90-462b-9fa6-38dba96e8d70",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T12:08:09.528705Z",
     "iopub.status.idle": "2025-04-21T12:08:09.529847Z",
     "shell.execute_reply": "2025-04-21T12:08:09.529489Z",
     "shell.execute_reply.started": "2025-04-21T12:08:09.529442Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745230671802,
     "user": {
      "displayName": "Mehdi MUNIM",
      "userId": "14326957258997738516"
     },
     "user_tz": -120
    },
    "id": "05dd6459-6d90-462b-9fa6-38dba96e8d70"
   },
   "outputs": [],
   "source": [
    "## possibilité d'ajouter si besoin de l'augmentation\n",
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.RandomFlip(\"horizontal\"),\n",
    "    keras.layers.RandomRotation(0.1),\n",
    "    keras.layers.RandomZoom(0.1)\n",
    "])\n",
    "\n",
    "def augmented_data_generator(images, masks, batch_size, num_classes):\n",
    "    \"\"\"\n",
    "    Crée un générateur de données augmentées en utilisant tf.data.Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def augment(image, mask):\n",
    "        \"\"\"Fonction pour appliquer l'augmentation de données.\"\"\"\n",
    "        augmented_image = data_augmentation(image)\n",
    "        return augmented_image, mask\n",
    "\n",
    "    # Créer le dataset à partir du générateur de données de base\n",
    "    dataset = data_generator(images, masks, batch_size, num_classes)\n",
    "\n",
    "    # Appliquer l'augmentation de données\n",
    "    dataset = dataset.map(augment)\n",
    "\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3211eb-a2a7-4ebe-ae27-43b83bf7343a",
   "metadata": {
    "id": "9b3211eb-a2a7-4ebe-ae27-43b83bf7343a"
   },
   "source": [
    "## Partie 6 : Entraînement des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ad9ba-cbbf-47b3-aaba-59e5e29a7d03",
   "metadata": {
    "id": "e14ad9ba-cbbf-47b3-aaba-59e5e29a7d03"
   },
   "source": [
    "### Configuration des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbffd02-01a5-4b1a-a0eb-e42d3a894d29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2025-04-21T12:08:09.531481Z",
     "iopub.status.idle": "2025-04-21T12:08:09.532595Z",
     "shell.execute_reply": "2025-04-21T12:08:09.532235Z",
     "shell.execute_reply.started": "2025-04-21T12:08:09.532201Z"
    },
    "executionInfo": {
     "elapsed": 7565,
     "status": "ok",
     "timestamp": 1745230679371,
     "user": {
      "displayName": "Mehdi MUNIM",
      "userId": "14326957258997738516"
     },
     "user_tz": -120
    },
    "id": "2bbffd02-01a5-4b1a-a0eb-e42d3a894d29",
    "outputId": "e95afe91-18ac-4659-effc-7c76dc8ca712"
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"UNet_mini\": build_unet_mini(IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES),\n",
    "    \"UNet_base\": build_unet(IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES),\n",
    "    \"VGG16_UNet\": build_vgg16_unet(IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES),\n",
    "    \"MobileNetV2_pretrained\": build_mobilenetv2_unet(IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc128fd-5813-46c7-b797-5a41ddb91be9",
   "metadata": {
    "id": "7bc128fd-5813-46c7-b797-5a41ddb91be9"
   },
   "source": [
    "### Configuration des loss (fonctions de perte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6151e77-e8a6-4408-adc7-aaf60be21c4a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T12:08:09.534130Z",
     "iopub.status.idle": "2025-04-21T12:08:09.535255Z",
     "shell.execute_reply": "2025-04-21T12:08:09.534908Z",
     "shell.execute_reply.started": "2025-04-21T12:08:09.534869Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745230679378,
     "user": {
      "displayName": "Mehdi MUNIM",
      "userId": "14326957258997738516"
     },
     "user_tz": -120
    },
    "id": "d6151e77-e8a6-4408-adc7-aaf60be21c4a"
   },
   "outputs": [],
   "source": [
    "\n",
    "losses = {\n",
    "    \"categorical_crossentropy\": keras.losses.CategoricalCrossentropy(),\n",
    "    \"dice_loss\": dice_loss,\n",
    "    \"mixed_loss\": mixed_loss\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0222a82-6832-4ecf-8be6-cb4660d21d28",
   "metadata": {
    "id": "a0222a82-6832-4ecf-8be6-cb4660d21d28"
   },
   "source": [
    "### Entraînement des modèles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d740093e-f8a1-4df5-8503-155d82c7f8e6",
   "metadata": {
    "id": "d740093e-f8a1-4df5-8503-155d82c7f8e6"
   },
   "source": [
    "1. Entraînement **sans** data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17a526-9af1-4c83-8989-58670933a673",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2025-04-21T12:08:09.537208Z",
     "iopub.status.idle": "2025-04-21T12:08:09.538410Z",
     "shell.execute_reply": "2025-04-21T12:08:09.538052Z",
     "shell.execute_reply.started": "2025-04-21T12:08:09.538015Z"
    },
    "id": "0b17a526-9af1-4c83-8989-58670933a673"
   },
   "outputs": [],
   "source": [
    "                                                                                                                                                                                                                                                  # Calculer steps_per_epoch\n",
    "steps_per_epoch = math.ceil(len(train_images) / BATCH_SIZE)\n",
    "\n",
    "# Calculer validation_steps\n",
    "validation_steps = math.ceil(len(val_images) / BATCH_SIZE)\n",
    "\n",
    "# Créer un exemple d'entrée\n",
    "input_example = np.random.rand(1, IMG_HEIGHT, IMG_WIDTH, 3).astype(np.float32)\n",
    "\n",
    "\n",
    "## On itère sur les modèles\n",
    "for model_name, model in models.items():\n",
    "\n",
    "\n",
    "    ## On itère (2e boucle) sur les loss\n",
    "    for loss_name, loss in losses.items():\n",
    "\n",
    "        registered_name = f\"{model_name}_{loss_name}\"\n",
    "        ## on sauve les résultats dans MFlow (model + loss)\n",
    "        with mlflow.start_run(run_name= registered_name):\n",
    "\n",
    "\n",
    "            ## génération des données d'entraînement\n",
    "            train_generator = data_generator(train_images,\n",
    "                                 train_masks,\n",
    "                                 BATCH_SIZE,\n",
    "                                 NUM_CLASSES)\n",
    "\n",
    "            ## génération des données de validation\n",
    "            val_generator = data_generator(val_images,\n",
    "                               val_masks,\n",
    "                               BATCH_SIZE,\n",
    "                               NUM_CLASSES)\n",
    "\n",
    "\n",
    "            ## Complilation\n",
    "            model.compile(optimizer='adam',\n",
    "                          loss=loss,\n",
    "                          metrics=[iou_metric, dice_coefficient])\n",
    "\n",
    "            ## Fitting\n",
    "\n",
    "\n",
    "            # Obtenir un exemple de sortie (prédiction)\n",
    "            output_example = model.predict(input_example)\n",
    "\n",
    "            # Déduire la signature\n",
    "            signature = infer_signature(input_example, output_example)\n",
    "\n",
    "            print(f\"fitting {model_name}_{loss_name}\")\n",
    "\n",
    "            #mlflow.keras.autolog()\n",
    "            model.fit(train_generator,\n",
    "                      validation_data=val_generator,\n",
    "                      epochs=EPOCHS,\n",
    "                     steps_per_epoch=steps_per_epoch,\n",
    "                     validation_steps = validation_steps)\n",
    "\n",
    "            # log model\n",
    "\n",
    "            mlflow.keras.log_model(model,\n",
    "                                   \"model\",\n",
    "                                   registered_model_name=registered_name,\n",
    "                                   signature=signature,\n",
    "                                   pip_requirements=[\"tensorflow\", \"keras\", \"opencv-python\"])\n",
    "\n",
    "\n",
    "            # Sauvegarde local du modèle\n",
    "            print(f\"Sauvegarde locale du modèle {registered_name}...\")\n",
    "\n",
    "            path = \"/content/drive/My Drive/OC/OC8/models\"\n",
    "            model_save_path = os.path.join(path, f\"{registered_name}.keras\")\n",
    "            model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c40eb-1b9d-42aa-a9bc-54d994b18f29",
   "metadata": {
    "id": "f28c40eb-1b9d-42aa-a9bc-54d994b18f29"
   },
   "source": [
    "2. Entraînement **avec** data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f700eb-2779-425d-bbae-c8ffb9debf3b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T12:08:09.540175Z",
     "iopub.status.idle": "2025-04-21T12:08:09.541353Z",
     "shell.execute_reply": "2025-04-21T12:08:09.540996Z",
     "shell.execute_reply.started": "2025-04-21T12:08:09.540963Z"
    },
    "id": "76f700eb-2779-425d-bbae-c8ffb9debf3b"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Créer un exemple d'entrée\n",
    "input_example = np.random.rand(1, IMG_HEIGHT, IMG_WIDTH, 3).astype(np.float32)\n",
    "\n",
    "\n",
    "# Entraînement avec augmentation de données\n",
    "for model_name, model in models.items():\n",
    "\n",
    "\n",
    "    for loss_name, loss in losses.items():\n",
    "\n",
    "        ## nom du modèle\n",
    "        registered_name = f\"{model_name}_{loss_name}_augmented\"\n",
    "\n",
    "\n",
    "        with mlflow.start_run(run_name=registered_name):\n",
    "\n",
    "            ## génération des données d'entraînement augmentées\n",
    "            augmented_train_generator = augmented_data_generator(train_images,\n",
    "                                                     train_masks,\n",
    "                                                     BATCH_SIZE,\n",
    "                                                     NUM_CLASSES)\n",
    "            ## génération des données de validation\n",
    "            val_generator = data_generator(val_images,\n",
    "                               val_masks,\n",
    "                               BATCH_SIZE,\n",
    "                               NUM_CLASSES)\n",
    "\n",
    "\n",
    "            model.compile(optimizer='adam',\n",
    "                          loss=loss,\n",
    "                          metrics=[iou_metric, dice_coefficient])\n",
    "\n",
    "            ## Fitting\n",
    "\n",
    "\n",
    "            # Obtenir un exemple de sortie (prédiction)\n",
    "            output_example = model.predict(input_example)\n",
    "\n",
    "            # Déduire la signature\n",
    "            signature = infer_signature(input_example, output_example)\n",
    "\n",
    "            print(f\"fitting {model_name}_{loss_name}_augmented\")\n",
    "\n",
    "            model.fit(augmented_train_generator,\n",
    "                      validation_data=val_generator,\n",
    "                      epochs=EPOCHS)\n",
    "\n",
    "            # log model\n",
    "\n",
    "            mlflow.keras.log_model(model,\n",
    "                                   \"model\",\n",
    "                                   registered_model_name=f\"{model_name}_{loss_name}_augmented\",\n",
    "                                   signature=signature,\n",
    "                                   pip_requirements=[\"tensorflow\", \"keras\", \"opencv-python\"])\n",
    "\n",
    "\n",
    "            # Sauvegarde local du modèle\n",
    "            print(f\"Sauvegarde locale du modèle {registered_name}...\")\n",
    "\n",
    "            path = \"/content/drive/My Drive/OC/OC8/models\"\n",
    "            model_save_path = os.path.join(path, f\"{registered_name}.keras\")\n",
    "            model.save(model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uCZ3qLTSrGnx",
   "metadata": {
    "id": "uCZ3qLTSrGnx"
   },
   "source": [
    "3. Optimisation d'un hyperparamètre (filter_size pour Unet-mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fqf5WkrB0R",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T12:08:09.543185Z",
     "iopub.status.idle": "2025-04-21T12:08:09.544373Z",
     "shell.execute_reply": "2025-04-21T12:08:09.544010Z",
     "shell.execute_reply.started": "2025-04-21T12:08:09.543970Z"
    },
    "id": "91fqf5WkrB0R"
   },
   "outputs": [],
   "source": [
    "# Définition des paramètres à tester\n",
    "filter_sizes = [8, 16, 32]  # Différentes tailles de filtres à tester (paramètre à optimiser)\n",
    "\n",
    "# Préparation des données (générateurs)\n",
    "train_generator = data_generator(train_images, train_masks, BATCH_SIZE, NUM_CLASSES)\n",
    "val_generator = data_generator(val_images, val_masks, BATCH_SIZE, NUM_CLASSES)\n",
    "steps_per_epoch = math.ceil(len(train_images) / BATCH_SIZE)\n",
    "validation_steps = math.ceil(len(val_images) / BATCH_SIZE)\n",
    "\n",
    "# Variables pour suivre le meilleur modèle\n",
    "best_model = None\n",
    "best_val_iou = 0.0\n",
    "best_filter_size = None\n",
    "\n",
    "# Boucle d'optimisation\n",
    "for filter_size in filter_sizes:\n",
    "    with mlflow.start_run(run_name=f\"UNetMini_FilterSize_{filter_size}\") as run:\n",
    "        print(f\"Training model with filter size: {filter_size}\")\n",
    "\n",
    "        # 1. Construction du modèle (avec le paramètre courant)\n",
    "        def build_unet_mini_custom(img_height, img_width, num_classes, filter_size):\n",
    "            inputs = keras.layers.Input(shape=(img_height, img_width, 3))\n",
    "\n",
    "            ## Bloc 1\n",
    "            conv1 = keras.layers.Conv2D(filter_size, (3, 3), activation='relu', padding='same')(inputs)\n",
    "            pool1 = keras.layers.MaxPooling2D((2, 2))(conv1)\n",
    "\n",
    "            ## Bloc 2\n",
    "            conv2 = keras.layers.Conv2D(filter_size*2, (3, 3), activation='relu', padding='same')(pool1)\n",
    "            pool2 = keras.layers.MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "            ## Bloc 3\n",
    "            conv3 = keras.layers.Conv2D(filter_size*4, (3, 3), activation='relu', padding='same')(pool2)\n",
    "\n",
    "            ## Bloc 4\n",
    "            up4 = keras.layers.UpSampling2D((2, 2))(conv3)\n",
    "            merge4 = keras.layers.concatenate([conv2, up4], axis=-1)\n",
    "            conv4 = keras.layers.Conv2D(filter_size*2, (3, 3), activation='relu', padding='same')(merge4)\n",
    "\n",
    "            ## Bloc 5\n",
    "            up5 = keras.layers.UpSampling2D((2, 2))(conv4)\n",
    "            merge5 = keras.layers.concatenate([conv1, up5], axis=-1)\n",
    "            conv5 = keras.layers.Conv2D(filter_size, (3, 3), activation='relu', padding='same')(merge5)\n",
    "\n",
    "            outputs = keras.layers.Conv2D(num_classes, (1, 1), activation='softmax')(conv5)\n",
    "\n",
    "            model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "            return model\n",
    "\n",
    "        model = build_unet_mini_custom(IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES, filter_size)\n",
    "\n",
    "        # 2. Compilation du modèle\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[iou_metric])\n",
    "\n",
    "        # 3. Entraînement du modèle\n",
    "        history = model.fit(train_generator,\n",
    "                            validation_data=val_generator,\n",
    "                            epochs=EPOCHS,\n",
    "                            steps_per_epoch=steps_per_epoch,\n",
    "                            validation_steps=validation_steps)\n",
    "\n",
    "        # 4. Evaluation du modèle (IoU sur la validation)\n",
    "        val_iou = history.history['val_iou_metric'][-1]\n",
    "        print(f\"Validation IoU: {val_iou}\")\n",
    "\n",
    "        # 5. Log des paramètres et métriques avec MLflow\n",
    "        mlflow.log_param(\"filter_size\", filter_size)\n",
    "        mlflow.log_metric(\"val_iou\", val_iou)\n",
    "\n",
    "        # 6. Sauvegarde du meilleur modèle\n",
    "        if val_iou > best_val_iou:\n",
    "            best_val_iou = val_iou\n",
    "            best_model = model\n",
    "            best_filter_size = filter_size\n",
    "\n",
    "            print(f\"Nouveau meilleur modèle trouvé avec filter_size={filter_size} et val_iou={best_val_iou}\")\n",
    "\n",
    "# Sauvegarde du meilleur modèle (en local)\n",
    "if best_model is not None:\n",
    "    best_model.save(\"best_unet_mini_model.keras\")\n",
    "    print(\"Meilleur modèle sauvegardé en local sous le nom : best_unet_mini_model.keras\")\n",
    "\n",
    "    # Log du meilleur modèle avec MLflow\n",
    "    with mlflow.start_run(run_name=\"BestUNetMiniModel\"): # Démarrer un nouveau run pour le meilleur modèle\n",
    "        mlflow.log_param(\"best_filter_size\", best_filter_size)\n",
    "        mlflow.log_metric(\"best_val_iou\", best_val_iou)\n",
    "\n",
    "        # Créer un exemple d'entrée pour la signature\n",
    "        input_example = np.random.rand(1, IMG_HEIGHT, IMG_WIDTH, 3).astype(np.float32)\n",
    "        output_example = best_model.predict(input_example)\n",
    "        signature = infer_signature(input_example, output_example)\n",
    "\n",
    "        mlflow.keras.log_model(best_model,\n",
    "                                \"model\",\n",
    "                                registered_model_name=\"BestUNetMini\", # Nom enregistré sur MLflow\n",
    "                                signature=signature,\n",
    "                                pip_requirements=[\"tensorflow\", \"keras\", \"opencv-python\"])\n",
    "\n",
    "        print(\"Meilleur modèle loggé avec MLflow.\")\n",
    "else:\n",
    "    print(\"Aucun modèle n'a été entraîné.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1h8tVEtgsDLO",
   "metadata": {
    "id": "1h8tVEtgsDLO"
   },
   "source": [
    "## Partie 7 : Évaluation des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2CPMDDOxvuQO",
   "metadata": {
    "id": "2CPMDDOxvuQO"
   },
   "source": [
    "### 1 - Évaluation des modèles sans augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gAWVLX5F_UPg",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T12:08:09.546144Z",
     "iopub.status.idle": "2025-04-21T12:08:09.547254Z",
     "shell.execute_reply": "2025-04-21T12:08:09.546922Z",
     "shell.execute_reply.started": "2025-04-21T12:08:09.546887Z"
    },
    "id": "gAWVLX5F_UPg"
   },
   "outputs": [],
   "source": [
    "# Créer une liste pour stocker les résultats\n",
    "results = []\n",
    "\n",
    "# Évaluation des modèles entraînés sans augmentation\n",
    "for model_name, model in models.items():\n",
    "    for loss_name, loss in losses.items():\n",
    "        test_generator = data_generator(test_images, test_masks, BATCH_SIZE, NUM_CLASSES)\n",
    "\n",
    "        search_result = mlflow.search_runs(filter_string=f'tags.mlflow.runName = \"{model_name}_{loss_name}\"')\n",
    "        model_path = f\"mlruns/839266598857507507/{search_result.iloc[0].run_id}/artifacts/model/data/model.keras\"\n",
    "\n",
    "        loaded_model = keras.models.load_model(model_path,\n",
    "                                               custom_objects={'iou_metric': iou_metric,\n",
    "                                                               'dice_loss': dice_loss,\n",
    "                                                               'mixed_loss': mixed_loss})\n",
    "\n",
    "        print(f\"{model_name}_{loss_name}\")\n",
    "        loss, iou = loaded_model.evaluate(test_generator)\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Loss': loss_name,\n",
    "            'Augmentation': 'No',\n",
    "            'Loss_Value': loss,\n",
    "            'IoU': iou\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nQN84rldvy1v",
   "metadata": {
    "id": "nQN84rldvy1v"
   },
   "source": [
    "### 2 - Évaluation des modèles avec augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wutSZ4XL_U2w",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T12:08:09.548930Z",
     "iopub.status.idle": "2025-04-21T12:08:09.550055Z",
     "shell.execute_reply": "2025-04-21T12:08:09.549732Z",
     "shell.execute_reply.started": "2025-04-21T12:08:09.549698Z"
    },
    "id": "wutSZ4XL_U2w"
   },
   "outputs": [],
   "source": [
    "# Évaluation des modèles entraînés avec augmentation de données\n",
    "for model_name, model in models.items():\n",
    "    for loss_name, loss in losses.items():\n",
    "        test_generator = data_generator(test_images, test_masks, BATCH_SIZE, NUM_CLASSES)\n",
    "\n",
    "        search_result = mlflow.search_runs(filter_string=f'tags.mlflow.runName = \"{model_name}_{loss_name}_augmented\"')\n",
    "        model_path = f\"mlruns/839266598857507507/{search_result.iloc[0].run_id}/artifacts/model/data/model.keras\"\n",
    "\n",
    "        loaded_model = keras.models.load_model(model_path,\n",
    "                                               custom_objects={'iou_metric': iou_metric,\n",
    "                                                               'dice_loss': dice_loss,\n",
    "                                                               'mixed_loss': mixed_loss})\n",
    "\n",
    "        print(f\"{model_name}_{loss_name}\")\n",
    "        loss, iou = loaded_model.evaluate(test_generator)\n",
    "        results.append({\n",
    "            'Model': model_name,\n",
    "            'Loss': loss_name,\n",
    "            'Augmentation': 'Yes',\n",
    "            'Loss_Value': loss,\n",
    "            'IoU': iou\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1_ZnZi3v7UJ",
   "metadata": {
    "id": "a1_ZnZi3v7UJ"
   },
   "source": [
    "### 3- Comparaison visuel des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HLjYJhG-_YJ8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T12:08:09.551774Z",
     "iopub.status.idle": "2025-04-21T12:08:09.552936Z",
     "shell.execute_reply": "2025-04-21T12:08:09.552581Z",
     "shell.execute_reply.started": "2025-04-21T12:08:09.552548Z"
    },
    "id": "HLjYJhG-_YJ8"
   },
   "outputs": [],
   "source": [
    "# Créer un DataFrame à partir des résultats\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# Créer un barplot pour comparer les IoU\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Model', y='IoU', hue='Augmentation', data=df_results)\n",
    "plt.title('Comparaison des IoU par modèle et augmentation')\n",
    "plt.xlabel('Modèle')\n",
    "plt.ylabel('IoU')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Augmentation')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Créer un barplot pour comparer les Loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Model', y='Loss_Value', hue='Augmentation', data=df_results)\n",
    "plt.title('Comparaison des Loss par modèle et augmentation')\n",
    "plt.xlabel('Modèle')\n",
    "plt.ylabel('Loss')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Augmentation')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hCSax7P_wEKu",
   "metadata": {
    "id": "hCSax7P_wEKu"
   },
   "source": [
    "### 4- Téléchargement des artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yj2eE0PKw1pL",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T12:08:09.554595Z",
     "iopub.status.idle": "2025-04-21T12:08:09.555717Z",
     "shell.execute_reply": "2025-04-21T12:08:09.555355Z",
     "shell.execute_reply.started": "2025-04-21T12:08:09.555319Z"
    },
    "id": "Yj2eE0PKw1pL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "experiment_id = \"839266598857507507\"  # Remplacez par l'ID de votre expérience\n",
    "\n",
    "# Chemin racine pour sauvegarder tous les artifacts localement\n",
    "base_dir = \"./mlflow_artifacts\"\n",
    "\n",
    "# Parcourir tous les runs de l'expérience et télécharger leurs artifacts\n",
    "for run in client.search_runs(experiment_ids=[experiment_id]):\n",
    "    run_id = run.info.run_id\n",
    "    local_dir = os.path.join(base_dir, run_id)\n",
    "    mlflow.artifacts.download_artifacts(artifact_uri=f\"runs:/{run_id}/\", dst_path=local_dir)\n",
    "    print(f\"Artifacts du run {run_id} téléchargés dans : {local_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1HIUgFIfwHgc",
   "metadata": {
    "id": "1HIUgFIfwHgc"
   },
   "source": [
    "### 5- Migration du mlflow artifact ==> Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wvPVJ1uyw7B5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-21T12:08:09.558148Z",
     "iopub.status.idle": "2025-04-21T12:08:09.559978Z",
     "shell.execute_reply": "2025-04-21T12:08:09.559488Z",
     "shell.execute_reply.started": "2025-04-21T12:08:09.559408Z"
    },
    "id": "wvPVJ1uyw7B5"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "# Chemin du dossier mlflow_artifacts\n",
    "source_dir = \"/content/mlflow_artifacts\"\n",
    "\n",
    "# Chemin vers Google Drive\n",
    "destination_dir = \"/content/drive/My Drive/OC/OC8/mlflow_artifacts\"\n",
    "\n",
    "# Copier le dossier\n",
    "shutil.copytree(source_dir, destination_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ba8d2c-b924-434f-83bb-db064550ae33",
   "metadata": {
    "id": "e4ba8d2c-b924-434f-83bb-db064550ae33"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce notebook a exploré différentes approches pour la segmentation d'images de scènes de rue. Les résultats montrent que l'augmentation de données peut améliorer les performances des modèles."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
