{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30dac86e-8894-44da-a3b2-7b19edffa1ac",
   "metadata": {},
   "source": [
    "# Notebook 3 : Modélisation\n",
    "\n",
    "Ce notebook a pour objectif de développer et comparer différents modèles de segmentation d'images pour identifier les objets présents dans des scènes de rue. Nous utiliserons le dataset Cityscapes, qui contient des images de haute qualité avec des annotations précises pour différents objets (voitures, piétons, bâtiments, etc.).\n",
    "Nous avons précédemment explorer les données (notebook 1), puis effectuer un prétraitement (notebook 2).\n",
    "\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "* Implémenter et comparer différents modèles de segmentation d'images (UNet, SegNet, etc.)\n",
    "* Tester différentes fonctions de perte :\n",
    "  - Entropie croisée (categorical cross-entropy)\n",
    "  - Dice loss\n",
    "  - Compromis entre les deux loss\n",
    "* Évaluer les performances des modèles avec différentes métriques\n",
    "    - Dice coefficient\n",
    "    - Intersection over Union (IoU)\n",
    "* Expérimenter avec l'augmentation de données pour améliorer la robustesse des modèles\n",
    "* Utiliser MLflow pour le suivi des expériences et la comparaison des résultats\n",
    "\n",
    "## Données\n",
    "\n",
    "Le dataset Cityscapes est utilisé pour ce projet. Il contient des images de scènes de rue avec des annotations pour 8 classes d'objets :\n",
    "- route (flat)\n",
    "- humain (human)\n",
    "- véhicule (vehicle)\n",
    "- bâtiment (construction)\n",
    "- objets (object)\n",
    "- nature (nature)\n",
    "- ciel (sky)\n",
    "- vide (void)\n",
    "  \n",
    "## Librairies\n",
    "\n",
    "* TensorFlow et Keras pour la construction et l'entraînement des modèles\n",
    "* MLflow pour le suivi des expériences\n",
    "* OpenCV pour le traitement des images\n",
    "* NumPy pour les opérations numériques\n",
    "* Matplotlib pour la visualisation des résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062db69-035d-47ae-b347-7571d2336c9e",
   "metadata": {},
   "source": [
    "## Partie 1 : Configuration de l'environnement et des paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "030ea610-f215-4910-b231-a82ce5297817",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T13:40:48.239559Z",
     "iopub.status.busy": "2025-03-14T13:40:48.238619Z",
     "iopub.status.idle": "2025-03-14T13:41:00.458668Z",
     "shell.execute_reply": "2025-03-14T13:41:00.457574Z",
     "shell.execute_reply.started": "2025-03-14T13:40:48.239508Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 14:40:49.853385: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-14 14:40:49.926711: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-14 14:40:50.013157: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741959650.124364   44309 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741959650.160029   44309 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-14 14:40:50.364894: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025/03/14 14:41:00 INFO mlflow.tracking.fluent: Experiment with name 'segmentation_images_cityscapes' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/mehdi/Documents/OC/OC8/notebooks/mlruns/571581462687747229', creation_time=1741959660447, experiment_id='571581462687747229', last_update_time=1741959660447, lifecycle_stage='active', name='segmentation_images_cityscapes', tags={}>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "\n",
    "# Configuration des chemins\n",
    "DATA_DIR = \"../data/processed\"\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(DATA_DIR, \"val\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test\")\n",
    "\n",
    "# Paramètres d'entraînement\n",
    "NUM_CLASSES = 8\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "\n",
    "## Tailles des images redimensionnées (identique au notebook 2)\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 256\n",
    "\n",
    "# Configuration de MLflow\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "mlflow.set_experiment(\"segmentation_images_cityscapes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfeb80f-051a-42e5-be7c-3354da82ea41",
   "metadata": {},
   "source": [
    "## Partie 2 : Chargement et préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bebf3ca0-e449-4c37-978f-78edb9be9a95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T13:41:00.460938Z",
     "iopub.status.busy": "2025-03-14T13:41:00.460293Z",
     "iopub.status.idle": "2025-03-14T13:41:00.551726Z",
     "shell.execute_reply": "2025-03-14T13:41:00.550532Z",
     "shell.execute_reply.started": "2025-03-14T13:41:00.460900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Chargement et préparation des données\n",
    "\n",
    "SAMPLE_SIZE = 10  # Nombre d'échantillons à utiliser pour l'entraînement et la validation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_data(data_dir):\n",
    "    images = sorted(glob.glob(os.path.join(data_dir, \"*\", \"*_image.png\")))\n",
    "    masks = sorted(glob.glob(os.path.join(data_dir, \"*\", \"*_mask.png\")))\n",
    "    return images, masks\n",
    "\n",
    "\n",
    "    \n",
    "## chargement des données preprocésées\n",
    "train_images, train_masks = load_data(TRAIN_DIR)\n",
    "val_images, val_masks = load_data(VAL_DIR)\n",
    "test_images, test_masks = load_data(TEST_DIR)\n",
    "\n",
    "## on ne garde qu'un échantillon limité à SAMPLE_SIZE maximum pour les données\n",
    "train_images = train_images[:SAMPLE_SIZE]\n",
    "train_masks = train_masks[:SAMPLE_SIZE]\n",
    "val_images = val_images[:SAMPLE_SIZE]\n",
    "val_masks = val_masks[:SAMPLE_SIZE]\n",
    "\n",
    "def data_generator(images, masks, batch_size, num_classes):\n",
    "    \"\"\"\n",
    "    Transforme un générateur de données (images et masques) en un tf.data.Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def generator():\n",
    "        \"\"\"Générateur pour le tf.data.Dataset.\"\"\"\n",
    "        for i in range(0, len(images)):\n",
    "            # Chargement de l'image\n",
    "            image = cv2.imread(images[i])\n",
    "            image = image / 255.0\n",
    "\n",
    "            # Chargement du masque en grayscale\n",
    "            mask = cv2.imread(masks[i], cv2.IMREAD_GRAYSCALE)\n",
    "            mask = keras.utils.to_categorical(mask, num_classes=num_classes)\n",
    "\n",
    "            yield image, mask\n",
    "\n",
    "    # Définir les types de données de sortie\n",
    "    output_types = (tf.float32, tf.float32)\n",
    "\n",
    "    # Définir les formes des données de sortie (None pour les dimensions variables)\n",
    "    # Ici, on suppose que toutes les images ont la même taille\n",
    "    # Si les images ont des tailles variables, il faudra adapter cette partie\n",
    "    image_shape = cv2.imread(images[0]).shape\n",
    "    mask_shape = (image_shape[0], image_shape[1], num_classes)\n",
    "\n",
    "    output_shapes = (tf.TensorShape(image_shape), tf.TensorShape(mask_shape))\n",
    "\n",
    "    # Créer le tf.data.Dataset à partir du générateur\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_types=output_types,\n",
    "        output_shapes=output_shapes\n",
    "    )\n",
    "\n",
    "    # Batching\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c2678d-0fa3-4e8b-8b26-57a44ae85fa4",
   "metadata": {},
   "source": [
    "## Partie 3 : Définition des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7443d9e-a575-4d8c-8403-0b04f519b63b",
   "metadata": {},
   "source": [
    "UNet mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c21c9f0-42fa-43d8-9158-0a24aa9213cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T13:41:00.557106Z",
     "iopub.status.busy": "2025-03-14T13:41:00.556394Z",
     "iopub.status.idle": "2025-03-14T13:41:00.571148Z",
     "shell.execute_reply": "2025-03-14T13:41:00.569112Z",
     "shell.execute_reply.started": "2025-03-14T13:41:00.557044Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_unet_mini(img_height, img_width, num_classes):\n",
    "    inputs = keras.layers.Input(shape=(img_height, img_width, 3))\n",
    "    \n",
    "    # Downsampling\n",
    "    conv1 = keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    pool1 = keras.layers.MaxPooling2D((2, 2))(conv1)\n",
    "    \n",
    "    conv2 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    pool2 = keras.layers.MaxPooling2D((2, 2))(conv2)\n",
    "    \n",
    "    # Bottleneck\n",
    "    conv3 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    \n",
    "    # Upsampling\n",
    "    up4 = keras.layers.UpSampling2D((2, 2))(conv3)\n",
    "    merge4 = keras.layers.concatenate([conv2, up4], axis=-1)\n",
    "    conv4 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(merge4)\n",
    "    \n",
    "    up5 = keras.layers.UpSampling2D((2, 2))(conv4)\n",
    "    merge5 = keras.layers.concatenate([conv1, up5], axis=-1)\n",
    "    conv5 = keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(merge5)\n",
    "    \n",
    "    # Output\n",
    "    outputs = keras.layers.Conv2D(num_classes, (1, 1), activation='softmax')(conv5)\n",
    "    \n",
    "    model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728eb44f-ff9a-420e-a04d-edee09721230",
   "metadata": {},
   "source": [
    "Modèle UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468b1579-2013-4754-9904-436ac8d6e2e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T13:41:00.574894Z",
     "iopub.status.busy": "2025-03-14T13:41:00.573211Z",
     "iopub.status.idle": "2025-03-14T13:41:00.610489Z",
     "shell.execute_reply": "2025-03-14T13:41:00.609035Z",
     "shell.execute_reply.started": "2025-03-14T13:41:00.574820Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_unet(img_height, img_width, num_classes):\n",
    "    \"\"\"\n",
    "    Définition du modèle UNET spécifique à la segmentation d'images.\n",
    "    U-Net : https://fr.wikipedia.org/wiki/U-Net\n",
    "    \"\"\"\n",
    "\n",
    "    ## Entrée\n",
    "    inputs = keras.layers.Input(shape=(img_height, img_width, 3))\n",
    "\n",
    "    ## Bloc 1\n",
    "    conv1 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    pool1 = keras.layers.MaxPooling2D((2, 2))(conv1)\n",
    "\n",
    "    ## Bloc 2\n",
    "    conv2 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    pool2 = keras.layers.MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "    ## Bloc 3\n",
    "    conv3 = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n",
    "\n",
    "    ## Bloc 4\n",
    "    up4 = keras.layers.UpSampling2D((2, 2))(conv3)\n",
    "    merge4 = keras.layers.concatenate([conv2, up4], axis=-1)\n",
    "    conv4 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(merge4)\n",
    "\n",
    "    ## Bloc 5\n",
    "    up5 = keras.layers.UpSampling2D((2, 2))(conv4)\n",
    "    merge5 = keras.layers.concatenate([conv1, up5], axis=-1)\n",
    "    conv5 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(merge5)\n",
    "\n",
    "    ## Couche de sortie\n",
    "    outputs = keras.layers.Conv2D(num_classes, (1, 1), activation='softmax')(conv5)\n",
    "    model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6fd16d-e9f2-4cc0-813f-a65952c83823",
   "metadata": {},
   "source": [
    "Modèle VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01001ed2-3cd8-41d9-87a4-2ae76eecde04",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T13:41:00.612625Z",
     "iopub.status.busy": "2025-03-14T13:41:00.611790Z",
     "iopub.status.idle": "2025-03-14T13:41:00.645011Z",
     "shell.execute_reply": "2025-03-14T13:41:00.643857Z",
     "shell.execute_reply.started": "2025-03-14T13:41:00.612584Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_vgg16_unet(img_height, img_width, num_classes):\n",
    "    \"\"\"\n",
    "    Construit un modèle VGG16-UNet avec des couches d'upsampling supplémentaires\n",
    "    pour s'assurer que la taille de la sortie correspond à la taille de l'entrée.\n",
    "    \"\"\"\n",
    "    # Charger le modèle VGG16 pré-entraîné sans la partie classification\n",
    "    vgg16 = keras.applications.VGG16(input_shape=(img_height, img_width, 3),\n",
    "                                     include_top=False,\n",
    "                                     weights='imagenet')\n",
    "\n",
    "    # Encoder (VGG16)\n",
    "    vgg16_output = vgg16.output\n",
    "\n",
    "    # Decoder (UNet-like)\n",
    "    # Ajouter des couches Conv2DTranspose supplémentaires pour augmenter la taille\n",
    "    up0 = keras.layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same')(vgg16_output) # Taille: H/16 * 2, W/16 * 2\n",
    "    merge0 = keras.layers.concatenate([vgg16.get_layer('block5_conv3').output, up0], axis=-1)\n",
    "    conv0 = keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(merge0)\n",
    "\n",
    "    up1 = keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv0) # Taille: H/8 * 2, W/8 * 2\n",
    "    merge1 = keras.layers.concatenate([vgg16.get_layer('block4_conv3').output, up1], axis=-1)\n",
    "    conv1 = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(merge1)\n",
    "\n",
    "    up2 = keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv1) # Taille: H/4 * 2, W/4 * 2\n",
    "    merge2 = keras.layers.concatenate([vgg16.get_layer('block3_conv3').output, up2], axis=-1)\n",
    "    conv2 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(merge2)\n",
    "\n",
    "    up3 = keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv2) # Taille: H/2 * 2, W/2 * 2\n",
    "    merge3 = keras.layers.concatenate([vgg16.get_layer('block2_conv2').output, up3], axis=-1)\n",
    "    conv3 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(merge3)\n",
    "\n",
    "    up4 = keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv3) # Taille: H * 2, W * 2\n",
    "    merge4 = keras.layers.concatenate([vgg16.get_layer('block1_conv2').output, up4], axis=-1)\n",
    "    conv4 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(merge4)\n",
    "\n",
    "    # Output\n",
    "    outputs = keras.layers.Conv2D(num_classes, (1, 1), activation='softmax')(conv4)\n",
    "\n",
    "    model = keras.models.Model(inputs=vgg16.input, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64b2096-50ce-4277-af23-f3a496746fc1",
   "metadata": {},
   "source": [
    "MobileNetV2_Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6329a360-55b1-4a6b-968b-6892e72a5f2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T13:41:00.646698Z",
     "iopub.status.busy": "2025-03-14T13:41:00.646048Z",
     "iopub.status.idle": "2025-03-14T13:41:00.669495Z",
     "shell.execute_reply": "2025-03-14T13:41:00.668305Z",
     "shell.execute_reply.started": "2025-03-14T13:41:00.646644Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_mobilenetv2_unet(img_height, img_width, num_classes):\n",
    "    \"\"\"Construit un modèle MobileNetV2-UNet.\"\"\"\n",
    "    # Charger le modèle MobileNetV2 pré-entraîné sans la partie classification\n",
    "    mobilenetv2 = keras.applications.MobileNetV2(input_shape=(img_height, img_width, 3),\n",
    "                                     include_top=False,\n",
    "                                     weights='imagenet')\n",
    "\n",
    "    # Récupérer la sortie de MobileNetV2\n",
    "    mobilenet_output = mobilenetv2.output\n",
    "\n",
    "    # Reshape de la sortie de MobileNetV2\n",
    "    reshape = keras.layers.Reshape((img_height // 32, img_width // 32, mobilenet_output.shape[-1]))(mobilenet_output)\n",
    "\n",
    "    # Decoder (UNet-like) avec 5 couches Conv2DTranspose\n",
    "    up1 = keras.layers.Conv2DTranspose(512, (3, 3), strides=(2, 2), padding='same')(reshape) # 8x16\n",
    "    conv1 = keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same')(up1)\n",
    "\n",
    "    up2 = keras.layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same')(conv1) # 16x32\n",
    "    conv2 = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(up2)\n",
    "\n",
    "    up3 = keras.layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same')(conv2) # 32x64\n",
    "    conv3 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(up3)\n",
    "\n",
    "    up4 = keras.layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same')(conv3) # 64x128\n",
    "    conv4 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(up4)\n",
    "\n",
    "    up5 = keras.layers.Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same')(conv4) # 128x256\n",
    "    conv5 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(up5)\n",
    "\n",
    "    # Output\n",
    "    outputs = keras.layers.Conv2D(num_classes, (1, 1), activation='softmax')(conv5)\n",
    "\n",
    "    model = keras.models.Model(inputs=mobilenetv2.input, outputs=outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e1e58-7efa-45d2-a06b-b162be7c245f",
   "metadata": {},
   "source": [
    "## Partie 4 : Définition des fonctions de perte et des métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a5940ff-33b8-4d42-91a7-9c68abc19c46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T13:41:00.671925Z",
     "iopub.status.busy": "2025-03-14T13:41:00.671159Z",
     "iopub.status.idle": "2025-03-14T13:41:00.715589Z",
     "shell.execute_reply": "2025-03-14T13:41:00.714651Z",
     "shell.execute_reply.started": "2025-03-14T13:41:00.671873Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## (1) Métriques\n",
    "\n",
    "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Calcule le coefficient de Dice, une métrique de similarité entre deux ensembles.\n",
    "    Il est souvent utilisé pour évaluer la performance des modèles de segmentation.\n",
    "\n",
    "    Args:\n",
    "        y_true (Tensor): Les valeurs de vérité terrain (ground truth).\n",
    "        y_pred (Tensor): Les prédictions du modèle.\n",
    "        smooth (float, optional): Un terme de lissage pour éviter la division par zéro. Defaults to 1e-6.\n",
    "\n",
    "    Returns:\n",
    "        float: Le coefficient de Dice, une valeur entre 0 et 1 inclusivement.\n",
    "\n",
    "    Plage de valeur :\n",
    "        - 0 : Indique une absence totale de chevauchement entre les prédictions et la vérité terrain.\n",
    "        - 1 : Indique un chevauchement parfait entre les prédictions et la vérité terrain (les ensembles sont identiques).\n",
    "    \"\"\"\n",
    "    y_true_f = keras.backend.flatten(tf.cast(y_true, tf.float32)) # Conversion de y_true en float32\n",
    "    y_pred_f = keras.backend.flatten(y_pred)\n",
    "    intersection = keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (keras.backend.sum(y_true_f) + keras.backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "def iou_metric(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Calcule l'IoU (Intersection over Union), également appelé indice de Jaccard,\n",
    "    une métrique couramment utilisée pour évaluer la performance des modèles de segmentation.\n",
    "\n",
    "    Args:\n",
    "        y_true (Tensor): Les valeurs de vérité terrain (ground truth).\n",
    "        y_pred (Tensor): Les prédictions du modèle.\n",
    "        smooth (float, optional): Un terme de lissage pour éviter la division par zéro. Defaults to 1e-6.\n",
    "\n",
    "    Returns:\n",
    "        float: L'IoU, une valeur entre 0 et 1 inclusivement.\n",
    "\n",
    "    Plage de valeur :\n",
    "        - 0 : Indique une absence totale de chevauchement entre les prédictions et la vérité terrain.\n",
    "        - 1 : Indique un chevauchement parfait entre les prédictions et la vérité terrain.\n",
    "    \"\"\"\n",
    "    y_true_f = keras.backend.flatten(tf.cast(y_true, tf.float32)) # Conversion de y_true en float32\n",
    "    y_pred_f = keras.backend.flatten(y_pred)\n",
    "    intersection = keras.backend.sum(y_true_f * y_pred_f)\n",
    "    union = keras.backend.sum(y_true_f) + keras.backend.sum(y_pred_f) - intersection\n",
    "    return (intersection + smooth) / (union + smooth)\n",
    "\n",
    "\n",
    "## (2) Fonctions de perte\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "def mixed_loss(y_true, y_pred):\n",
    "    \"\"\"Fonction de perte combinant categorical crossentropy et dice loss.\"\"\"\n",
    "    return 0.5 * keras.losses.CategoricalCrossentropy()(y_true, y_pred) + 0.5 * dice_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ad58f-684e-4213-b4fd-252b62292506",
   "metadata": {},
   "source": [
    "## Partie 5 : Augmentation des données (Data Augmentation)\n",
    "\n",
    "On génère des données \"augmentées\", à savoir en faisant des modifications légères (retournement, rotation, zoom...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05dd6459-6d90-462b-9fa6-38dba96e8d70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T13:41:00.717269Z",
     "iopub.status.busy": "2025-03-14T13:41:00.716769Z",
     "iopub.status.idle": "2025-03-14T13:41:00.832773Z",
     "shell.execute_reply": "2025-03-14T13:41:00.831069Z",
     "shell.execute_reply.started": "2025-03-14T13:41:00.717225Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 14:41:00.749356: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.RandomFlip(\"horizontal\"),\n",
    "    keras.layers.RandomRotation(0.1),\n",
    "    keras.layers.RandomZoom(0.1)\n",
    "])\n",
    "\n",
    "def augmented_data_generator(images, masks, batch_size, num_classes):\n",
    "    \"\"\"\n",
    "    Crée un générateur de données augmentées en utilisant tf.data.Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def augment(image, mask):\n",
    "        \"\"\"Fonction pour appliquer l'augmentation de données.\"\"\"\n",
    "        augmented_image = data_augmentation(image)\n",
    "        return augmented_image, mask\n",
    "\n",
    "    # Créer le dataset à partir du générateur de données de base\n",
    "    dataset = data_generator(images, masks, batch_size, num_classes)\n",
    "\n",
    "    # Appliquer l'augmentation de données\n",
    "    dataset = dataset.map(augment)\n",
    "\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3211eb-a2a7-4ebe-ae27-43b83bf7343a",
   "metadata": {},
   "source": [
    "## Partie 6 : Entraînement des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ad9ba-cbbf-47b3-aaba-59e5e29a7d03",
   "metadata": {},
   "source": [
    "### Configuration des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bbffd02-01a5-4b1a-a0eb-e42d3a894d29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T13:41:00.836519Z",
     "iopub.status.busy": "2025-03-14T13:41:00.835252Z",
     "iopub.status.idle": "2025-03-14T13:41:04.313161Z",
     "shell.execute_reply": "2025-03-14T13:41:04.311043Z",
     "shell.execute_reply.started": "2025-03-14T13:41:00.836463Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 14:41:01.682349: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 18874368 exceeds 10% of free system memory.\n",
      "2025-03-14 14:41:01.712001: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 18874368 exceeds 10% of free system memory.\n",
      "2025-03-14 14:41:01.723432: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 18874368 exceeds 10% of free system memory.\n",
      "/tmp/ipykernel_44309/1192823791.py:4: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "  mobilenetv2 = keras.applications.MobileNetV2(input_shape=(img_height, img_width, 3),\n",
      "2025-03-14 14:41:04.023168: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 23592960 exceeds 10% of free system memory.\n",
      "2025-03-14 14:41:04.057912: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 23592960 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"UNet_mini\": build_unet_mini(IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES),\n",
    "    \"UNet_base\": build_unet(IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES),\n",
    "    \"VGG16_UNet\": build_vgg16_unet(IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES),\n",
    "    \"MobileNetV2_pretrained\": build_mobilenetv2_unet(IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc128fd-5813-46c7-b797-5a41ddb91be9",
   "metadata": {},
   "source": [
    "### Configuration des loss (fonctions de perte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6151e77-e8a6-4408-adc7-aaf60be21c4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-14T13:41:04.320586Z",
     "iopub.status.busy": "2025-03-14T13:41:04.315567Z",
     "iopub.status.idle": "2025-03-14T13:41:04.343492Z",
     "shell.execute_reply": "2025-03-14T13:41:04.341917Z",
     "shell.execute_reply.started": "2025-03-14T13:41:04.320508Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "losses = {\n",
    "    \"categorical_crossentropy\": keras.losses.CategoricalCrossentropy(),\n",
    "    \"dice_loss\": dice_loss,\n",
    "    \"mixed_loss\": mixed_loss\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0222a82-6832-4ecf-8be6-cb4660d21d28",
   "metadata": {},
   "source": [
    "### Entraînement des modèles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d740093e-f8a1-4df5-8503-155d82c7f8e6",
   "metadata": {},
   "source": [
    "Entraînement **sans** data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17a526-9af1-4c83-8989-58670933a673",
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-14T13:51:00.394Z",
     "iopub.execute_input": "2025-03-14T13:41:04.345423Z",
     "iopub.status.busy": "2025-03-14T13:41:04.344715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 370ms/step\n",
      "fitting UNet_mini_categorical_crossentropy\n",
      "Epoch 1/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 961ms/step - iou_metric: 0.0665 - loss: 2.0851 - val_iou_metric: 0.0700 - val_loss: 2.0382\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 14:41:11.251346: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/home/mehdi/Documents/OC/OC8/.venv/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - iou_metric: 0.0000e+00 - loss: 0.0000e+00 - val_iou_metric: 0.0700 - val_loss: 2.0382\n",
      "Epoch 3/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 810ms/step - iou_metric: 0.0719 - loss: 2.0169 - val_iou_metric: 0.0782 - val_loss: 1.9744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'UNet_mini_categorical_crossentropy'.\n",
      "Created version '1' of model 'UNet_mini_categorical_crossentropy'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468ms/step\n",
      "fitting UNet_mini_dice_loss\n",
      "Epoch 1/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 712ms/step - iou_metric: 0.0833 - loss: 0.8475 - val_iou_metric: 0.1029 - val_loss: 0.8126\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 14:41:23.235357: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - iou_metric: 0.0000e+00 - loss: 0.0000e+00 - val_iou_metric: 0.1029 - val_loss: 0.8126\n",
      "Epoch 3/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 845ms/step - iou_metric: 0.1184 - loss: 0.7918 - val_iou_metric: 0.1648 - val_loss: 0.7185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'UNet_mini_dice_loss'.\n",
      "Created version '1' of model 'UNet_mini_dice_loss'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 267ms/step\n",
      "fitting UNet_mini_mixed_loss\n",
      "Epoch 1/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 779ms/step - iou_metric: 0.1513 - loss: 1.3361 - val_iou_metric: 0.1076 - val_loss: 1.4067\n",
      "Epoch 2/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - iou_metric: 0.0000e+00 - loss: 0.0000e+00 - val_iou_metric: 0.1076 - val_loss: 1.4067\n",
      "Epoch 3/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 558ms/step - iou_metric: 0.1120 - loss: 1.2903 - val_iou_metric: 0.1113 - val_loss: 1.3884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'UNet_mini_mixed_loss'.\n",
      "Created version '1' of model 'UNet_mini_mixed_loss'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 414ms/step\n",
      "fitting UNet_base_categorical_crossentropy\n",
      "Epoch 1/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 9s/step - iou_metric: 0.0730 - loss: 2.0205 - val_iou_metric: 0.1278 - val_loss: 1.9169\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 14:42:00.138740: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - iou_metric: 0.0000e+00 - loss: 0.0000e+00 - val_iou_metric: 0.1278 - val_loss: 1.9169\n",
      "Epoch 3/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 8s/step - iou_metric: 0.1481 - loss: 1.6649 - val_iou_metric: 0.1399 - val_loss: 2.1106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'UNet_base_categorical_crossentropy'.\n",
      "Created version '1' of model 'UNet_base_categorical_crossentropy'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fc269577e20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "fitting UNet_base_dice_loss\n",
      "Epoch 1/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 7s/step - iou_metric: 0.2035 - loss: 0.6711 - val_iou_metric: 0.2522 - val_loss: 0.6002\n",
      "Epoch 2/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - iou_metric: 0.0000e+00 - loss: 0.0000e+00 - val_iou_metric: 0.2522 - val_loss: 0.6002\n",
      "Epoch 3/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4s/step - iou_metric: 0.2697 - loss: 0.5750 - val_iou_metric: 0.2523 - val_loss: 0.6001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'UNet_base_dice_loss'.\n",
      "Created version '1' of model 'UNet_base_dice_loss'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fc269185120> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 421ms/step\n",
      "fitting UNet_base_mixed_loss\n",
      "Epoch 1/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 5s/step - iou_metric: 0.2699 - loss: 7.7378 - val_iou_metric: 0.2150 - val_loss: 2.0238\n",
      "Epoch 2/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 999ms/step - iou_metric: 0.0000e+00 - loss: 0.0000e+00 - val_iou_metric: 0.2150 - val_loss: 2.0238\n",
      "Epoch 3/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8s/step - iou_metric: 0.1897 - loss: 1.3372 - val_iou_metric: 0.0889 - val_loss: 1.3545\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'UNet_base_mixed_loss'.\n",
      "Created version '1' of model 'UNet_base_mixed_loss'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 933ms/step\n",
      "fitting VGG16_UNet_categorical_crossentropy\n",
      "Epoch 1/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 8s/step - iou_metric: 0.1204 - loss: 3.9259 - val_iou_metric: 0.1456 - val_loss: 2.4179\n",
      "Epoch 2/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - iou_metric: 0.0000e+00 - loss: 0.0000e+00 - val_iou_metric: 0.1456 - val_loss: 2.4179\n",
      "Epoch 3/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8s/step - iou_metric: 0.1888 - loss: 1.8240 - val_iou_metric: 0.1172 - val_loss: 1.9559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'VGG16_UNet_categorical_crossentropy'.\n",
      "Created version '1' of model 'VGG16_UNet_categorical_crossentropy'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 701ms/step\n",
      "fitting VGG16_UNet_dice_loss\n",
      "Epoch 1/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 5s/step - iou_metric: 0.1980 - loss: 0.6805 - val_iou_metric: 0.2523 - val_loss: 0.6000\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 14:45:42.924192: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - iou_metric: 0.0000e+00 - loss: 0.0000e+00 - val_iou_metric: 0.2523 - val_loss: 0.6000\n",
      "Epoch 3/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 8s/step - iou_metric: 0.2697 - loss: 0.5750 - val_iou_metric: 0.2561 - val_loss: 0.5949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'VGG16_UNet_dice_loss'.\n",
      "Created version '1' of model 'VGG16_UNet_dice_loss'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 556ms/step\n",
      "fitting VGG16_UNet_mixed_loss\n",
      "Epoch 1/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 7s/step - iou_metric: 0.2527 - loss: 5.0676 - val_iou_metric: 0.2051 - val_loss: 1.4159\n",
      "Epoch 2/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 989ms/step - iou_metric: 0.0000e+00 - loss: 0.0000e+00 - val_iou_metric: 0.2051 - val_loss: 1.4159\n",
      "Epoch 3/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 8s/step - iou_metric: 0.2044 - loss: 1.2111 - val_iou_metric: 0.1396 - val_loss: 1.2630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'VGG16_UNet_mixed_loss'.\n",
      "Created version '1' of model 'VGG16_UNet_mixed_loss'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "fitting MobileNetV2_pretrained_categorical_crossentropy\n",
      "Epoch 1/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3s/step - iou_metric: 0.0749 - loss: 2.0480 - val_iou_metric: 0.2611 - val_loss: 13.0621\n",
      "Epoch 2/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 283ms/step - iou_metric: 0.0000e+00 - loss: 0.0000e+00 - val_iou_metric: 0.2611 - val_loss: 13.0621\n",
      "Epoch 3/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - iou_metric: 0.2105 - loss: 5.2202 - val_iou_metric: 0.0889 - val_loss: 1.9526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'MobileNetV2_pretrained_categorical_crossentropy'.\n",
      "Created version '1' of model 'MobileNetV2_pretrained_categorical_crossentropy'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "fitting MobileNetV2_pretrained_dice_loss\n",
      "Epoch 1/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 3s/step - iou_metric: 0.1807 - loss: 0.7187 - val_iou_metric: 0.2784 - val_loss: 0.5661\n",
      "Epoch 2/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 374ms/step - iou_metric: 0.0000e+00 - loss: 0.0000e+00 - val_iou_metric: 0.2784 - val_loss: 0.5661\n",
      "Epoch 3/3\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - iou_metric: 0.2985 - loss: 0.5418 - val_iou_metric: 0.2737 - val_loss: 0.5717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'MobileNetV2_pretrained_dice_loss'.\n",
      "Created version '1' of model 'MobileNetV2_pretrained_dice_loss'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "fitting MobileNetV2_pretrained_mixed_loss\n",
      "Epoch 1/3\n",
      "\u001b[1m1/3\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 44s/step - iou_metric: 0.3042 - loss: 23.6420"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Calculer steps_per_epoch\n",
    "steps_per_epoch = math.ceil(len(train_images) / BATCH_SIZE)\n",
    "\n",
    "# Calculer validation_steps\n",
    "validation_steps = math.ceil(len(val_images) / BATCH_SIZE)\n",
    "\n",
    "# Créer un exemple d'entrée\n",
    "input_example = np.random.rand(1, IMG_HEIGHT, IMG_WIDTH, 3).astype(np.float32)\n",
    "\n",
    "\n",
    "## On itère sur les modèles\n",
    "for model_name, model in models.items():\n",
    "    ## On itère (2e boucle) sur les loss\n",
    "    for loss_name, loss in losses.items():\n",
    "        ## on sauve les résultats dans MFlow (model + loss)\n",
    "        with mlflow.start_run(run_name=f\"{model_name}_{loss_name}\"):\n",
    "\n",
    "            \n",
    "            ## génération des données d'entraînement\n",
    "            train_generator = data_generator(train_images,\n",
    "                                 train_masks,\n",
    "                                 BATCH_SIZE,\n",
    "                                 NUM_CLASSES)\n",
    "\n",
    "            ## génération des données de validation\n",
    "            val_generator = data_generator(val_images,\n",
    "                               val_masks,\n",
    "                               BATCH_SIZE,\n",
    "                               NUM_CLASSES)\n",
    "\n",
    "\n",
    "            ## Complilation\n",
    "            model.compile(optimizer='adam', loss=loss, metrics=[iou_metric])\n",
    "\n",
    "            ## Fitting\n",
    "\n",
    "            \n",
    "            # Obtenir un exemple de sortie (prédiction)\n",
    "            output_example = model.predict(input_example)\n",
    "\n",
    "            # Déduire la signature\n",
    "            signature = infer_signature(input_example, output_example)\n",
    "            \n",
    "            print(f\"fitting {model_name}_{loss_name}\")\n",
    "\n",
    "            #mlflow.keras.autolog()\n",
    "            model.fit(train_generator,\n",
    "                      validation_data=val_generator,\n",
    "                      epochs=EPOCHS,\n",
    "                     steps_per_epoch=steps_per_epoch,\n",
    "                     validation_steps = validation_steps)\n",
    "\n",
    "            # log model\n",
    "\n",
    "            mlflow.keras.log_model(model,\n",
    "                                   \"model\",\n",
    "                                   registered_model_name=f\"{model_name}_{loss_name}\",\n",
    "                                   signature=signature,\n",
    "                                   pip_requirements=[\"tensorflow\", \"keras\", \"opencv-python\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c40eb-1b9d-42aa-a9bc-54d994b18f29",
   "metadata": {},
   "source": [
    "Entraînement **avec** data augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f700eb-2779-425d-bbae-c8ffb9debf3b",
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-14T13:51:00.396Z"
    }
   },
   "outputs": [],
   "source": [
    "# Entraînement avec augmentation de données\n",
    "for model_name, model in models.items():\n",
    "    for loss_name, loss in losses.items():\n",
    "        with mlflow.start_run(run_name=f\"{model_name}_{loss_name}_augmented\"):\n",
    "\n",
    "            ## génération des données d'entraînement augmentées\n",
    "            augmented_train_generator = augmented_data_generator(train_images,\n",
    "                                                     train_masks,\n",
    "                                                     BATCH_SIZE,\n",
    "                                                     NUM_CLASSES)\n",
    "            ## génération des données de validation\n",
    "            val_generator = data_generator(val_images,\n",
    "                               val_masks,\n",
    "                               BATCH_SIZE,\n",
    "                               NUM_CLASSES)\n",
    "\n",
    "            \n",
    "            model.compile(optimizer='adam', loss=loss, metrics=[iou_metric])\n",
    "            \n",
    "            print(f\"fitting {model_name}_{loss_name}_augmented\")\n",
    "            \n",
    "            model.fit(augmented_train_generator,\n",
    "                      validation_data=val_generator,\n",
    "                      epochs=EPOCHS)\n",
    "\n",
    "            # log model\n",
    "            mlflow.keras.log_model(model,\n",
    "                                   \"model\",\n",
    "                                   registered_model_name=f\"{model_name}_{loss_name}_augmented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f247ce3-d6e6-43fe-bae0-d1b7732a4a2c",
   "metadata": {},
   "source": [
    "## Partie 7 : Évaluation des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f05fd-acff-40ca-832b-afe8bd4e0d6b",
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-14T13:51:00.405Z"
    }
   },
   "outputs": [],
   "source": [
    "# Évaluation des modèles entraînés\n",
    "for model_name, model in models.items():\n",
    "    for loss_name, loss in losses.items():\n",
    "\n",
    "\n",
    "        ## génération des données de test\n",
    "        test_generator = data_generator(test_images,test_masks,\n",
    "                                                     BATCH_SIZE,\n",
    "                                                     NUM_CLASSES)\n",
    "        # Chargement du modèle entraîné\n",
    "        model_path = f\"mlruns/0/{mlflow.search_runs(filter_string=f'tags.mlflow.runName = \\'{model_name}_{loss_name}\\'').iloc[0].run_id}/artifacts/model/data/model.keras\"\n",
    "        loaded_model = keras.models.load_model(model_path,\n",
    "                                               custom_objects={'iou_metric': iou_metric, 'dice_loss': dice_loss})\n",
    "\n",
    "        # Évaluation du modèle sur l'ensemble de test\n",
    "        loss, iou = loaded_model.evaluate(test_generator)\n",
    "        print(f\"Modèle {model_name} avec perte {loss_name} : Loss = {loss}, IoU = {iou}\")\n",
    "\n",
    "# Évaluation des modèles entraînés avec augmentation de données\n",
    "for model_name, model in models.items():\n",
    "    for loss_name, loss in losses.items():\n",
    "\n",
    "        ## génération des données de test\n",
    "        test_generator = data_generator(test_images,test_masks,\n",
    "                                                     BATCH_SIZE,\n",
    "                                                     NUM_CLASSES)\n",
    "        \n",
    "        # Chargement du modèle entraîné\n",
    "        model_path = f\"mlruns/0/{mlflow.search_runs(filter_string=f'tags.mlflow.runName = \\'{model_name}_{loss_name}_augmented\\'').iloc[0].run_id}/artifacts/model/data/model.keras\"\n",
    "        loaded_model = keras.models.load_model(model_path, custom_objects={'iou_metric': iou_metric, 'dice_loss': dice_loss})\n",
    "\n",
    "        # Évaluation du modèle sur l'ensemble de test\n",
    "        loss, iou = loaded_model.evaluate(test_generator)\n",
    "        print(f\"Modèle {model_name} avec perte {loss_name} et augmentation : Loss = {loss}, IoU = {iou}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ba8d2c-b924-434f-83bb-db064550ae33",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce notebook a exploré différentes approches pour la segmentation d'images de scènes de rue. Les résultats montrent que l'augmentation de données peut améliorer les performances des modèles. \n",
    "\n",
    "### Améliorations possibles\n",
    "\n",
    "* Tester d'autres modèles de segmentation, tels que SegNet ou DeepLabv3.\n",
    "* Expérimenter avec différentes fonctions de perte et métriques.\n",
    "* Utiliser des techniques d'augmentation de données plus avancées.\n",
    "* Optimiser les hyperparamètres des modèles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
