{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30dac86e-8894-44da-a3b2-7b19edffa1ac",
   "metadata": {},
   "source": [
    "# Notebook 3 : Modélisation\n",
    "\n",
    "Ce notebook a pour objectif de développer et comparer différents modèles de segmentation d'images pour identifier les objets présents dans des scènes de rue. Nous utiliserons le dataset Cityscapes, qui contient des images de haute qualité avec des annotations précises pour différents objets (voitures, piétons, bâtiments, etc.).\n",
    "Nous avons précédemment explorer les données (notebook 1), puis effectuer un prétraitement (notebook 2).\n",
    "\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "* Implémenter et comparer différents modèles de segmentation d'images (UNet, SegNet, etc.)\n",
    "* Tester différentes fonctions de perte :\n",
    "  - Entropie croisée (categorical cross-entropy)\n",
    "  - Dice loss\n",
    "  - Compromis entre les deux loss\n",
    "* Évaluer les performances des modèles avec différentes métriques\n",
    "    - Dice coefficient\n",
    "    - Intersection over Union (IoU)\n",
    "* Expérimenter avec l'augmentation de données pour améliorer la robustesse des modèles\n",
    "* Utiliser MLflow pour le suivi des expériences et la comparaison des résultats\n",
    "\n",
    "## Données\n",
    "\n",
    "Le dataset Cityscapes est utilisé pour ce projet. Il contient des images de scènes de rue avec des annotations pour 8 classes d'objets :\n",
    "- route (flat)\n",
    "- humain (human)\n",
    "- véhicule (vehicle)\n",
    "- bâtiment (construction)\n",
    "- objets (object)\n",
    "- nature (nature)\n",
    "- ciel (sky)\n",
    "- vide (void)\n",
    "  \n",
    "## Librairies\n",
    "\n",
    "* TensorFlow et Keras pour la construction et l'entraînement des modèles\n",
    "* MLflow pour le suivi des expériences\n",
    "* OpenCV pour le traitement des images\n",
    "* NumPy pour les opérations numériques\n",
    "* Matplotlib pour la visualisation des résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062db69-035d-47ae-b347-7571d2336c9e",
   "metadata": {},
   "source": [
    "## Partie 1 : Configuration de l'environnement et des paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "030ea610-f215-4910-b231-a82ce5297817",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T16:08:13.060598Z",
     "iopub.status.busy": "2025-03-02T16:08:13.060215Z",
     "iopub.status.idle": "2025-03-02T16:08:18.818967Z",
     "shell.execute_reply": "2025-03-02T16:08:18.817303Z",
     "shell.execute_reply.started": "2025-03-02T16:08:13.060566Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 17:08:13.968666: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-02 17:08:13.972622: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-02 17:08:13.984788: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740931694.005103   31084 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740931694.011115   31084 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-02 17:08:14.033045: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/mehdi/Documents/OC/OC8/notebooks/mlruns/846683678405741605', creation_time=1740849750954, experiment_id='846683678405741605', last_update_time=1740849750954, lifecycle_stage='active', name='segmentation_images_cityscapes', tags={}>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "\n",
    "\n",
    "# Configuration des chemins\n",
    "DATA_DIR = \"../data/processed\"\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(DATA_DIR, \"val\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test\")\n",
    "\n",
    "# Paramètres d'entraînement\n",
    "NUM_CLASSES = 8\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "\n",
    "## Tailles des images redimensionnées (identique au notebook 2)\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 256\n",
    "\n",
    "# Configuration de MLflow\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "mlflow.set_experiment(\"segmentation_images_cityscapes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfeb80f-051a-42e5-be7c-3354da82ea41",
   "metadata": {},
   "source": [
    "## Partie 2 : Chargement et préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bebf3ca0-e449-4c37-978f-78edb9be9a95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T16:08:18.821434Z",
     "iopub.status.busy": "2025-03-02T16:08:18.820445Z",
     "iopub.status.idle": "2025-03-02T16:08:18.880370Z",
     "shell.execute_reply": "2025-03-02T16:08:18.878537Z",
     "shell.execute_reply.started": "2025-03-02T16:08:18.821370Z"
    }
   },
   "outputs": [],
   "source": [
    "# Chargement et préparation des données\n",
    "\n",
    "SAMPLE_SIZE = 50  # Nombre d'échantillons à utiliser pour l'entraînement et la validation\n",
    "\n",
    "def load_data(data_dir):\n",
    "    images = sorted(glob.glob(os.path.join(data_dir, \"*\", \"*_image.png\")))\n",
    "    masks = sorted(glob.glob(os.path.join(data_dir, \"*\", \"*_mask.png\")))\n",
    "    return images, masks\n",
    "\n",
    "\n",
    "    \n",
    "## chargement des données preprocésées\n",
    "train_images, train_masks = load_data(TRAIN_DIR)\n",
    "val_images, val_masks = load_data(VAL_DIR)\n",
    "test_images, test_masks = load_data(TEST_DIR)\n",
    "\n",
    "## on ne garde qu'un échantillon limité à SAMLE_SIZE maximum pour les données\n",
    "train_images = train_images[:SAMPLE_SIZE]\n",
    "train_masks = train_masks[:SAMPLE_SIZE]\n",
    "val_images = val_images[:SAMPLE_SIZE]\n",
    "val_masks = val_masks[:SAMPLE_SIZE]\n",
    "\n",
    "def data_generator(images, masks, batch_size, num_classes):\n",
    "    \"\"\"\n",
    "    Transforme un générateur de données (images et masques) en un tf.data.Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def generator():\n",
    "        \"\"\"Générateur pour le tf.data.Dataset.\"\"\"\n",
    "        for i in range(0, len(images)):\n",
    "            # Chargement de l'image\n",
    "            image = cv2.imread(images[i])\n",
    "            image = image / 255.0\n",
    "\n",
    "            # Chargement du masque en grayscale\n",
    "            mask = cv2.imread(masks[i], cv2.IMREAD_GRAYSCALE)\n",
    "            mask = keras.utils.to_categorical(mask, num_classes=num_classes)\n",
    "\n",
    "            yield image, mask\n",
    "\n",
    "    # Définir les types de données de sortie\n",
    "    output_types = (tf.float32, tf.float32)\n",
    "\n",
    "    # Définir les formes des données de sortie (None pour les dimensions variables)\n",
    "    # Ici, on suppose que toutes les images ont la même taille\n",
    "    # Si les images ont des tailles variables, il faudra adapter cette partie\n",
    "    image_shape = cv2.imread(images[0]).shape\n",
    "    mask_shape = (image_shape[0], image_shape[1], num_classes)\n",
    "\n",
    "    output_shapes = (tf.TensorShape(image_shape), tf.TensorShape(mask_shape))\n",
    "\n",
    "    # Créer le tf.data.Dataset à partir du générateur\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_types=output_types,\n",
    "        output_shapes=output_shapes\n",
    "    )\n",
    "\n",
    "    # Batching\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c2678d-0fa3-4e8b-8b26-57a44ae85fa4",
   "metadata": {},
   "source": [
    "## Partie 3 : Définition des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd64d004-887f-47ef-91d4-31b4e7c30bd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T16:08:18.886033Z",
     "iopub.status.busy": "2025-03-02T16:08:18.885235Z",
     "iopub.status.idle": "2025-03-02T16:08:18.907788Z",
     "shell.execute_reply": "2025-03-02T16:08:18.906650Z",
     "shell.execute_reply.started": "2025-03-02T16:08:18.885986Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_unet(img_height, img_width, num_classes):\n",
    "    \"\"\"\n",
    "    Définition du modèle UNET spécifique à la segmentation d'images.\n",
    "    U-Net : https://fr.wikipedia.org/wiki/U-Net\n",
    "    \"\"\"\n",
    "\n",
    "    ## Entrée\n",
    "    inputs = keras.layers.Input(shape=(img_height, img_width, 3))\n",
    "\n",
    "    ## Bloc 1\n",
    "    conv1 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    pool1 = keras.layers.MaxPooling2D((2, 2))(conv1)\n",
    "\n",
    "    ## Bloc 2\n",
    "    conv2 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    pool2 = keras.layers.MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "    ## Bloc 3\n",
    "    conv3 = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n",
    "\n",
    "    ## Bloc 4\n",
    "    up4 = keras.layers.UpSampling2D((2, 2))(conv3)\n",
    "    merge4 = keras.layers.concatenate([conv2, up4], axis=-1)\n",
    "    conv4 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(merge4)\n",
    "\n",
    "    ## Bloc 5\n",
    "    up5 = keras.layers.UpSampling2D((2, 2))(conv4)\n",
    "    merge5 = keras.layers.concatenate([conv1, up5], axis=-1)\n",
    "    conv5 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(merge5)\n",
    "\n",
    "    ## Couche de sortie\n",
    "    outputs = keras.layers.Conv2D(num_classes, (1, 1), activation='softmax')(conv5)\n",
    "    model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_unet_mini(img_height, img_width, num_classes):\n",
    "    inputs = keras.layers.Input(shape=(img_height, img_width, 3))\n",
    "    \n",
    "    # Downsampling\n",
    "    conv1 = keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    pool1 = keras.layers.MaxPooling2D((2, 2))(conv1)\n",
    "    \n",
    "    conv2 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    pool2 = keras.layers.MaxPooling2D((2, 2))(conv2)\n",
    "    \n",
    "    # Bottleneck\n",
    "    conv3 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    \n",
    "    # Upsampling\n",
    "    up4 = keras.layers.UpSampling2D((2, 2))(conv3)\n",
    "    merge4 = keras.layers.concatenate([conv2, up4], axis=-1)\n",
    "    conv4 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(merge4)\n",
    "    \n",
    "    up5 = keras.layers.UpSampling2D((2, 2))(conv4)\n",
    "    merge5 = keras.layers.concatenate([conv1, up5], axis=-1)\n",
    "    conv5 = keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(merge5)\n",
    "    \n",
    "    # Output\n",
    "    outputs = keras.layers.Conv2D(num_classes, (1, 1), activation='softmax')(conv5)\n",
    "    \n",
    "    model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_vgg16_unet(img_height, img_width, num_classes):\n",
    "    vgg16 = keras.applications.VGG16(input_shape=(img_height, img_width, 3), include_top=False, weights='imagenet')\n",
    "    \n",
    "    # Encoder (VGG16)\n",
    "    vgg16_output = vgg16.output\n",
    "    \n",
    "    # Decoder (UNet-like)\n",
    "    up1 = keras.layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same')(vgg16_output)\n",
    "    merge1 = keras.layers.concatenate([vgg16.get_layer('block5_conv3').output, up1], axis=-1)\n",
    "    conv1 = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(merge1)\n",
    "    \n",
    "    up2 = keras.layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same')(conv1)\n",
    "    merge2 = keras.layers.concatenate([vgg16.get_layer('block4_conv3').output, up2], axis=-1)\n",
    "    conv2 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(merge2)\n",
    "    \n",
    "    up3 = keras.layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same')(conv2)\n",
    "    merge3 = keras.layers.concatenate([vgg16.get_layer('block3_conv3').output, up3], axis=-1)\n",
    "    conv3 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(merge3)\n",
    "    \n",
    "    # Output\n",
    "    outputs = keras.layers.Conv2D(num_classes, (1, 1), activation='softmax')(conv3)\n",
    "    \n",
    "    model = keras.models.Model(inputs=vgg16.input, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e1e58-7efa-45d2-a06b-b162be7c245f",
   "metadata": {},
   "source": [
    "## Partie 4 : Définition des fonctions de perte et des métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a5940ff-33b8-4d42-91a7-9c68abc19c46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T16:08:18.909888Z",
     "iopub.status.busy": "2025-03-02T16:08:18.908847Z",
     "iopub.status.idle": "2025-03-02T16:08:18.934478Z",
     "shell.execute_reply": "2025-03-02T16:08:18.933375Z",
     "shell.execute_reply.started": "2025-03-02T16:08:18.909821Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## (1) Métriques\n",
    "\n",
    "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Métrique proche du F1-score\n",
    "    \"\"\"\n",
    "    y_true_f = keras.backend.flatten(tf.cast(y_true, tf.float32)) # Conversion de y_true en float32\n",
    "    y_pred_f = keras.backend.flatten(y_pred)\n",
    "    intersection = keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (keras.backend.sum(y_true_f) + keras.backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "def iou_metric(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_f = keras.backend.flatten(tf.cast(y_true, tf.float32)) # Conversion de y_true en float32\n",
    "    y_pred_f = keras.backend.flatten(y_pred)\n",
    "    intersection = keras.backend.sum(y_true_f * y_pred_f)\n",
    "    union = keras.backend.sum(y_true_f) + keras.backend.sum(y_pred_f) - intersection\n",
    "    return (intersection + smooth) / (union + smooth)\n",
    "\n",
    "\n",
    "## (2) Fonctions de perte\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "### les autres fonctions de perte seront définis à même le modèle \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ad58f-684e-4213-b4fd-252b62292506",
   "metadata": {},
   "source": [
    "## Partie 5 : Augmentation des données (Data Augmentation)\n",
    "\n",
    "On génère des données \"augmentées\", à savoir en faisant des modifications légères (retournement, rotation, zoom...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05dd6459-6d90-462b-9fa6-38dba96e8d70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T16:08:18.935786Z",
     "iopub.status.busy": "2025-03-02T16:08:18.935400Z",
     "iopub.status.idle": "2025-03-02T16:08:19.000549Z",
     "shell.execute_reply": "2025-03-02T16:08:18.999013Z",
     "shell.execute_reply.started": "2025-03-02T16:08:18.935753Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 17:08:18.960625: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.RandomFlip(\"horizontal\"),\n",
    "    keras.layers.RandomRotation(0.1),\n",
    "    keras.layers.RandomZoom(0.1)\n",
    "])\n",
    "\n",
    "def augmented_data_generator(images, masks, batch_size, img_height, img_width, num_classes):\n",
    "    for batch_images, batch_masks in data_generator(images, masks, batch_size, img_height, img_width, num_classes):\n",
    "        augmented_images = data_augmentation(batch_images)\n",
    "        yield augmented_images, batch_masks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3211eb-a2a7-4ebe-ae27-43b83bf7343a",
   "metadata": {},
   "source": [
    "## Partie 6 : Entraînement des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ad9ba-cbbf-47b3-aaba-59e5e29a7d03",
   "metadata": {},
   "source": [
    "### Configuration des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bbffd02-01a5-4b1a-a0eb-e42d3a894d29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T16:08:19.002664Z",
     "iopub.status.busy": "2025-03-02T16:08:19.001863Z",
     "iopub.status.idle": "2025-03-02T16:08:19.711139Z",
     "shell.execute_reply": "2025-03-02T16:08:19.709931Z",
     "shell.execute_reply.started": "2025-03-02T16:08:19.002626Z"
    }
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"UNet_base\": build_unet(IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES),\n",
    "    \"UNet_mini\": build_unet_mini(IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES),\n",
    "    \"VGG16_UNet\": build_vgg16_unet(IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES),\n",
    "    #\"MobileNetV2_pretrained\": keras.applications.MobileNetV2(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), include_top=False, weights='imagenet')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc128fd-5813-46c7-b797-5a41ddb91be9",
   "metadata": {},
   "source": [
    "### Configuration des loss (fonctions de perte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6151e77-e8a6-4408-adc7-aaf60be21c4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T16:08:19.720277Z",
     "iopub.status.busy": "2025-03-02T16:08:19.715121Z",
     "iopub.status.idle": "2025-03-02T16:08:19.728062Z",
     "shell.execute_reply": "2025-03-02T16:08:19.726763Z",
     "shell.execute_reply.started": "2025-03-02T16:08:19.720031Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "losses = {\n",
    "    \"categorical_crossentropy\": keras.losses.CategoricalCrossentropy(),\n",
    "    \"dice_loss\": dice_loss,\n",
    "    \"mixed_loss\": lambda y_true, y_pred: 0.5 * keras.losses.CategoricalCrossentropy()(y_true, y_pred) + 0.5 * dice_loss(y_true, y_pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0222a82-6832-4ecf-8be6-cb4660d21d28",
   "metadata": {},
   "source": [
    "### Entraînement des modèles\n",
    "\n",
    "- Sans data augmentation\n",
    "\n",
    "- Avec data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f700eb-2779-425d-bbae-c8ffb9debf3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T16:08:19.731366Z",
     "iopub.status.busy": "2025-03-02T16:08:19.730116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting UNet_base_categorical_crossentropy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 17:08:20.371201: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-03-02 17:08:20.693094: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 17:08:24.675873: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 226492416 exceeds 10% of free system memory.\n",
      "2025-03-02 17:08:24.677830: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 226492416 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1/Unknown \u001b[1m7s\u001b[0m 7s/step - iou_metric: 0.0669 - loss: 2.0767"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 17:08:28.461496: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 226492416 exceeds 10% of free system memory.\n",
      "2025-03-02 17:08:28.463489: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 226492416 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2/Unknown \u001b[1m10s\u001b[0m 3s/step - iou_metric: 0.0689 - loss: 2.0521"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 17:08:32.074827: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 226492416 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     13/Unknown \u001b[1m42s\u001b[0m 3s/step - iou_metric: 0.0980 - loss: 1.9046"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehdi/Documents/OC/OC8/.venv/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n",
      "2025-03-02 17:09:16.493263: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 4s/step - iou_metric: 0.0984 - loss: 1.8997 - val_iou_metric: 0.1425 - val_loss: 1.7680\n",
      "Epoch 2/3\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - iou_metric: 0.1402 - loss: 1.6457   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 4s/step - iou_metric: 0.1398 - loss: 1.6456 - val_iou_metric: 0.1613 - val_loss: 1.6024\n",
      "Epoch 3/3\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "# Calculer steps_per_epoch\n",
    "steps_per_epoch = math.ceil(len(train_images) / BATCH_SIZE)\n",
    "\n",
    "# Calculer validation_steps\n",
    "validation_steps = math.ceil(len(val_images) / BATCH_SIZE)\n",
    "\n",
    "\n",
    "## On itère sur les modèles\n",
    "for model_name, model in models.items():\n",
    "    ## On itère (2e boucle) sur les loss\n",
    "    for loss_name, loss in losses.items():\n",
    "        ## on sauve les résultats dans MFlow (model + loss)\n",
    "        with mlflow.start_run(run_name=f\"{model_name}_{loss_name}\"):\n",
    "\n",
    "            \n",
    "            ## génération des données d'entraînement\n",
    "            train_generator = data_generator(train_images,\n",
    "                                 train_masks,\n",
    "                                 BATCH_SIZE,\n",
    "                                 NUM_CLASSES)\n",
    "\n",
    "            ## génération des données de validation\n",
    "            val_generator = data_generator(val_images,\n",
    "                               val_masks,\n",
    "                               BATCH_SIZE,\n",
    "                               NUM_CLASSES)\n",
    "\n",
    "            ## fine-tuning du modèle préentraîné \n",
    "            if \"pretrained\" in model_name:\n",
    "                x = model.output\n",
    "                \n",
    "                ## ajout d'un décodeur pour la segmentation\n",
    "                x = keras.layers.Conv2DTranspose(256, 3, strides=2, padding='same')(x)\n",
    "                x = keras.layers.Conv2DTranspose(128, 3, strides=2, padding='same')(x)\n",
    "                outputs = keras.layers.Conv2D(NUM_CLASSES, 1, activation='softmax')(x)\n",
    "                model = keras.Model(inputs=model.input, outputs=outputs)\n",
    "            model.compile(optimizer='adam', loss=loss, metrics=[iou_metric])\n",
    "            mlflow.tensorflow.autolog()\n",
    "\n",
    "            print(f\"fitting {model_name}_{loss_name}\")\n",
    "            model.fit(train_generator,\n",
    "                      validation_data=val_generator,\n",
    "                      epochs=EPOCHS,\n",
    "                     steps_per_epoch=steps_per_epoch,\n",
    "                     validation_steps = validation_steps)\n",
    "\n",
    "# Entraînement avec augmentation de données\n",
    "for model_name, model in models.items():\n",
    "    for loss_name, loss in losses.items():\n",
    "        with mlflow.start_run(run_name=f\"{model_name}_{loss_name}_augmented\"):\n",
    "\n",
    "            ## génération des données d'entraînement augmentées\n",
    "            augmented_train_generator = augmented_data_generator(train_images,\n",
    "                                                     train_masks,\n",
    "                                                     BATCH_SIZE,\n",
    "                                                     IMG_HEIGHT,\n",
    "                                                     IMG_WIDTH,\n",
    "                                                     NUM_CLASSES)\n",
    "            ## génération des données de validation\n",
    "            val_generator = data_generator(val_images,\n",
    "                               val_masks,\n",
    "                               BATCH_SIZE,\n",
    "                               NUM_CLASSES)\n",
    "\n",
    "            \n",
    "            if \"pretrained\" in model_name:\n",
    "                \n",
    "                ## ajout d'un décodeur pour la segmentation\n",
    "                x = model.output\n",
    "                x = keras.layers.Conv2DTranspose(256, 3, strides=2, padding='same')(x)\n",
    "                x = keras.layers.Conv2DTranspose(128, 3, strides=2, padding='same')(x)\n",
    "                outputs = keras.layers.Conv2D(NUM_CLASSES, 1, activation='softmax')(x)\n",
    "                model = keras.Model(inputs=model.input, outputs=outputs)\n",
    "            model.compile(optimizer='adam', loss=loss, metrics=[iou_metric])\n",
    "            mlflow.tensorflow.autolog()\n",
    "\n",
    "            print(f\"fitting {model_name}_{loss_name}_augmented\")\n",
    "            model.fit(augmented_train_generator,\n",
    "                      validation_data=val_generator,\n",
    "                      epochs=EPOCHS,\n",
    "                     steps_per_epoch=steps_per_epoch,\n",
    "                     validation_steps = validation_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f247ce3-d6e6-43fe-bae0-d1b7732a4a2c",
   "metadata": {},
   "source": [
    "## Partie 7 : Évaluation des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f05fd-acff-40ca-832b-afe8bd4e0d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation des modèles entraînés\n",
    "for model_name, model in models.items():\n",
    "    for loss_name, loss in losses.items():\n",
    "\n",
    "\n",
    "        ## génération des données de test\n",
    "        test_generator = data_generator(test_images,test_masks,\n",
    "                                                     BATCH_SIZE,\n",
    "                                                     IMG_HEIGHT,\n",
    "                                                     IMG_WIDTH,\n",
    "                                                     NUM_CLASSES)\n",
    "        # Chargement du modèle entraîné\n",
    "        model_path = f\"mlruns/0/{mlflow.search_runs(filter_string=f'tags.mlflow.runName = \\'{model_name}_{loss_name}\\'').iloc[0].run_id}/artifacts/model/data/model.keras\"\n",
    "        loaded_model = keras.models.load_model(model_path, custom_objects={'iou_metric': iou_metric, 'dice_loss': dice_loss})\n",
    "\n",
    "        # Évaluation du modèle sur l'ensemble de test\n",
    "        loss, iou = loaded_model.evaluate(test_generator)\n",
    "        print(f\"Modèle {model_name} avec perte {loss_name} : Loss = {loss}, IoU = {iou}\")\n",
    "\n",
    "# Évaluation des modèles entraînés avec augmentation de données\n",
    "for model_name, model in models.items():\n",
    "    for loss_name, loss in losses.items():\n",
    "\n",
    "        ## génération des données de test\n",
    "        test_generator = data_generator(test_images,test_masks,\n",
    "                                                     BATCH_SIZE,\n",
    "                                                     IMG_HEIGHT,\n",
    "                                                     IMG_WIDTH,\n",
    "                                                     NUM_CLASSES)\n",
    "        \n",
    "        # Chargement du modèle entraîné\n",
    "        model_path = f\"mlruns/0/{mlflow.search_runs(filter_string=f'tags.mlflow.runName = \\'{model_name}_{loss_name}_augmented\\'').iloc[0].run_id}/artifacts/model/data/model.keras\"\n",
    "        loaded_model = keras.models.load_model(model_path, custom_objects={'iou_metric': iou_metric, 'dice_loss': dice_loss})\n",
    "\n",
    "        # Évaluation du modèle sur l'ensemble de test\n",
    "        loss, iou = loaded_model.evaluate(test_generator)\n",
    "        print(f\"Modèle {model_name} avec perte {loss_name} et augmentation : Loss = {loss}, IoU = {iou}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ba8d2c-b924-434f-83bb-db064550ae33",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce notebook a exploré différentes approches pour la segmentation d'images de scènes de rue. Les résultats montrent que l'augmentation de données peut améliorer les performances des modèles. \n",
    "\n",
    "### Améliorations possibles\n",
    "\n",
    "* Tester d'autres modèles de segmentation, tels que SegNet ou DeepLabv3.\n",
    "* Expérimenter avec différentes fonctions de perte et métriques.\n",
    "* Utiliser des techniques d'augmentation de données plus avancées.\n",
    "* Optimiser les hyperparamètres des modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13142c69-6342-4a17-93c3-182b596cf61b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
