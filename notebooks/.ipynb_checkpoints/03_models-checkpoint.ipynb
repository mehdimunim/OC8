{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30dac86e-8894-44da-a3b2-7b19edffa1ac",
   "metadata": {},
   "source": [
    "# Notebook 3 : Modélisation\n",
    "\n",
    "Ce notebook a pour objectif de développer et comparer différents modèles de segmentation d'images pour identifier les objets présents dans des scènes de rue. Nous utiliserons le dataset Cityscapes, qui contient des images de haute qualité avec des annotations précises pour différents objets (voitures, piétons, bâtiments, etc.).\n",
    "Nous avons précédemment explorer les données (notebook 1), puis effectuer un prétraitement (notebook 2).\n",
    "\n",
    "\n",
    "## Objectifs\n",
    "\n",
    "* Implémenter et comparer différents modèles de segmentation d'images (UNet, SegNet, etc.)\n",
    "* Tester différentes fonctions de perte :\n",
    "  - Entropie croisée (categorical cross-entropy)\n",
    "  - Dice loss\n",
    "  - Compromis entre les deux loss\n",
    "* Évaluer les performances des modèles avec différentes métriques\n",
    "    - Dice coefficient\n",
    "    - Intersection over Union (IoU)\n",
    "* Expérimenter avec l'augmentation de données pour améliorer la robustesse des modèles\n",
    "* Utiliser MLflow pour le suivi des expériences et la comparaison des résultats\n",
    "\n",
    "## Données\n",
    "\n",
    "Le dataset Cityscapes est utilisé pour ce projet. Il contient des images de scènes de rue avec des annotations pour 8 classes d'objets :\n",
    "- route (flat)\n",
    "- humain (human)\n",
    "- véhicule (vehicle)\n",
    "- bâtiment (construction)\n",
    "- objets (object)\n",
    "- nature (nature)\n",
    "- ciel (sky)\n",
    "- vide (void)\n",
    "  \n",
    "## Librairies\n",
    "\n",
    "* TensorFlow et Keras pour la construction et l'entraînement des modèles\n",
    "* MLflow pour le suivi des expériences\n",
    "* OpenCV pour le traitement des images\n",
    "* NumPy pour les opérations numériques\n",
    "* Matplotlib pour la visualisation des résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062db69-035d-47ae-b347-7571d2336c9e",
   "metadata": {},
   "source": [
    "## Partie 1 : Configuration de l'environnement et des paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "030ea610-f215-4910-b231-a82ce5297817",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T16:39:08.559327Z",
     "iopub.status.busy": "2025-03-02T16:39:08.558998Z",
     "iopub.status.idle": "2025-03-02T16:39:14.595457Z",
     "shell.execute_reply": "2025-03-02T16:39:14.594382Z",
     "shell.execute_reply.started": "2025-03-02T16:39:08.559295Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 17:39:09.400180: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-02 17:39:09.404267: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-02 17:39:09.416137: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740933549.436536   35022 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740933549.442540   35022 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-02 17:39:09.463468: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/mehdi/Documents/OC/OC8/notebooks/mlruns/846683678405741605', creation_time=1740849750954, experiment_id='846683678405741605', last_update_time=1740849750954, lifecycle_stage='active', name='segmentation_images_cityscapes', tags={}>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "\n",
    "\n",
    "# Configuration des chemins\n",
    "DATA_DIR = \"../data/processed\"\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "VAL_DIR = os.path.join(DATA_DIR, \"val\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test\")\n",
    "\n",
    "# Paramètres d'entraînement\n",
    "NUM_CLASSES = 8\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 3\n",
    "\n",
    "## Tailles des images redimensionnées (identique au notebook 2)\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 256\n",
    "\n",
    "# Configuration de MLflow\n",
    "mlflow.set_tracking_uri(\"file:./mlruns\")\n",
    "mlflow.set_experiment(\"segmentation_images_cityscapes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfeb80f-051a-42e5-be7c-3354da82ea41",
   "metadata": {},
   "source": [
    "## Partie 2 : Chargement et préparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bebf3ca0-e449-4c37-978f-78edb9be9a95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T16:39:14.598192Z",
     "iopub.status.busy": "2025-03-02T16:39:14.597194Z",
     "iopub.status.idle": "2025-03-02T16:39:14.730612Z",
     "shell.execute_reply": "2025-03-02T16:39:14.729296Z",
     "shell.execute_reply.started": "2025-03-02T16:39:14.598124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Chargement et préparation des données\n",
    "\n",
    "SAMPLE_SIZE = 50  # Nombre d'échantillons à utiliser pour l'entraînement et la validation\n",
    "\n",
    "def load_data(data_dir):\n",
    "    images = sorted(glob.glob(os.path.join(data_dir, \"*\", \"*_image.png\")))\n",
    "    masks = sorted(glob.glob(os.path.join(data_dir, \"*\", \"*_mask.png\")))\n",
    "    return images, masks\n",
    "\n",
    "\n",
    "    \n",
    "## chargement des données preprocésées\n",
    "train_images, train_masks = load_data(TRAIN_DIR)\n",
    "val_images, val_masks = load_data(VAL_DIR)\n",
    "test_images, test_masks = load_data(TEST_DIR)\n",
    "\n",
    "## on ne garde qu'un échantillon limité à SAMLE_SIZE maximum pour les données\n",
    "train_images = train_images[:SAMPLE_SIZE]\n",
    "train_masks = train_masks[:SAMPLE_SIZE]\n",
    "val_images = val_images[:SAMPLE_SIZE]\n",
    "val_masks = val_masks[:SAMPLE_SIZE]\n",
    "\n",
    "def data_generator(images, masks, batch_size, num_classes):\n",
    "    \"\"\"\n",
    "    Transforme un générateur de données (images et masques) en un tf.data.Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def generator():\n",
    "        \"\"\"Générateur pour le tf.data.Dataset.\"\"\"\n",
    "        for i in range(0, len(images)):\n",
    "            # Chargement de l'image\n",
    "            image = cv2.imread(images[i])\n",
    "            image = image / 255.0\n",
    "\n",
    "            # Chargement du masque en grayscale\n",
    "            mask = cv2.imread(masks[i], cv2.IMREAD_GRAYSCALE)\n",
    "            mask = keras.utils.to_categorical(mask, num_classes=num_classes)\n",
    "\n",
    "            yield image, mask\n",
    "\n",
    "    # Définir les types de données de sortie\n",
    "    output_types = (tf.float32, tf.float32)\n",
    "\n",
    "    # Définir les formes des données de sortie (None pour les dimensions variables)\n",
    "    # Ici, on suppose que toutes les images ont la même taille\n",
    "    # Si les images ont des tailles variables, il faudra adapter cette partie\n",
    "    image_shape = cv2.imread(images[0]).shape\n",
    "    mask_shape = (image_shape[0], image_shape[1], num_classes)\n",
    "\n",
    "    output_shapes = (tf.TensorShape(image_shape), tf.TensorShape(mask_shape))\n",
    "\n",
    "    # Créer le tf.data.Dataset à partir du générateur\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_types=output_types,\n",
    "        output_shapes=output_shapes\n",
    "    )\n",
    "\n",
    "    # Batching\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c2678d-0fa3-4e8b-8b26-57a44ae85fa4",
   "metadata": {},
   "source": [
    "## Partie 3 : Définition des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd64d004-887f-47ef-91d4-31b4e7c30bd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T16:39:14.734320Z",
     "iopub.status.busy": "2025-03-02T16:39:14.733576Z",
     "iopub.status.idle": "2025-03-02T16:39:14.759812Z",
     "shell.execute_reply": "2025-03-02T16:39:14.758756Z",
     "shell.execute_reply.started": "2025-03-02T16:39:14.734260Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_unet(img_height, img_width, num_classes):\n",
    "    \"\"\"\n",
    "    Définition du modèle UNET spécifique à la segmentation d'images.\n",
    "    U-Net : https://fr.wikipedia.org/wiki/U-Net\n",
    "    \"\"\"\n",
    "\n",
    "    ## Entrée\n",
    "    inputs = keras.layers.Input(shape=(img_height, img_width, 3))\n",
    "\n",
    "    ## Bloc 1\n",
    "    conv1 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    pool1 = keras.layers.MaxPooling2D((2, 2))(conv1)\n",
    "\n",
    "    ## Bloc 2\n",
    "    conv2 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    pool2 = keras.layers.MaxPooling2D((2, 2))(conv2)\n",
    "\n",
    "    ## Bloc 3\n",
    "    conv3 = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(pool2)\n",
    "\n",
    "    ## Bloc 4\n",
    "    up4 = keras.layers.UpSampling2D((2, 2))(conv3)\n",
    "    merge4 = keras.layers.concatenate([conv2, up4], axis=-1)\n",
    "    conv4 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(merge4)\n",
    "\n",
    "    ## Bloc 5\n",
    "    up5 = keras.layers.UpSampling2D((2, 2))(conv4)\n",
    "    merge5 = keras.layers.concatenate([conv1, up5], axis=-1)\n",
    "    conv5 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(merge5)\n",
    "\n",
    "    ## Couche de sortie\n",
    "    outputs = keras.layers.Conv2D(num_classes, (1, 1), activation='softmax')(conv5)\n",
    "    model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_unet_mini(img_height, img_width, num_classes):\n",
    "    inputs = keras.layers.Input(shape=(img_height, img_width, 3))\n",
    "    \n",
    "    # Downsampling\n",
    "    conv1 = keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    pool1 = keras.layers.MaxPooling2D((2, 2))(conv1)\n",
    "    \n",
    "    conv2 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    pool2 = keras.layers.MaxPooling2D((2, 2))(conv2)\n",
    "    \n",
    "    # Bottleneck\n",
    "    conv3 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    \n",
    "    # Upsampling\n",
    "    up4 = keras.layers.UpSampling2D((2, 2))(conv3)\n",
    "    merge4 = keras.layers.concatenate([conv2, up4], axis=-1)\n",
    "    conv4 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(merge4)\n",
    "    \n",
    "    up5 = keras.layers.UpSampling2D((2, 2))(conv4)\n",
    "    merge5 = keras.layers.concatenate([conv1, up5], axis=-1)\n",
    "    conv5 = keras.layers.Conv2D(16, (3, 3), activation='relu', padding='same')(merge5)\n",
    "    \n",
    "    # Output\n",
    "    outputs = keras.layers.Conv2D(num_classes, (1, 1), activation='softmax')(conv5)\n",
    "    \n",
    "    model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_vgg16_unet(img_height, img_width, num_classes):\n",
    "    vgg16 = keras.applications.VGG16(input_shape=(img_height, img_width, 3), include_top=False, weights='imagenet')\n",
    "    \n",
    "    # Encoder (VGG16)\n",
    "    vgg16_output = vgg16.output\n",
    "    \n",
    "    # Decoder (UNet-like)\n",
    "    up1 = keras.layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same')(vgg16_output)\n",
    "    merge1 = keras.layers.concatenate([vgg16.get_layer('block5_conv3').output, up1], axis=-1)\n",
    "    conv1 = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same')(merge1)\n",
    "    \n",
    "    up2 = keras.layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same')(conv1)\n",
    "    merge2 = keras.layers.concatenate([vgg16.get_layer('block4_conv3').output, up2], axis=-1)\n",
    "    conv2 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same')(merge2)\n",
    "    \n",
    "    up3 = keras.layers.Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same')(conv2)\n",
    "    merge3 = keras.layers.concatenate([vgg16.get_layer('block3_conv3').output, up3], axis=-1)\n",
    "    conv3 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(merge3)\n",
    "    \n",
    "    # Output\n",
    "    outputs = keras.layers.Conv2D(num_classes, (1, 1), activation='softmax')(conv3)\n",
    "    \n",
    "    model = keras.models.Model(inputs=vgg16.input, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7e1e58-7efa-45d2-a06b-b162be7c245f",
   "metadata": {},
   "source": [
    "## Partie 4 : Définition des fonctions de perte et des métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a5940ff-33b8-4d42-91a7-9c68abc19c46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T16:39:14.761836Z",
     "iopub.status.busy": "2025-03-02T16:39:14.761066Z",
     "iopub.status.idle": "2025-03-02T16:39:14.792091Z",
     "shell.execute_reply": "2025-03-02T16:39:14.789838Z",
     "shell.execute_reply.started": "2025-03-02T16:39:14.761764Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## (1) Métriques\n",
    "\n",
    "def dice_coefficient(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Métrique proche du F1-score\n",
    "    \"\"\"\n",
    "    y_true_f = keras.backend.flatten(tf.cast(y_true, tf.float32)) # Conversion de y_true en float32\n",
    "    y_pred_f = keras.backend.flatten(y_pred)\n",
    "    intersection = keras.backend.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (keras.backend.sum(y_true_f) + keras.backend.sum(y_pred_f) + smooth)\n",
    "\n",
    "def iou_metric(y_true, y_pred, smooth=1e-6):\n",
    "    y_true_f = keras.backend.flatten(tf.cast(y_true, tf.float32)) # Conversion de y_true en float32\n",
    "    y_pred_f = keras.backend.flatten(y_pred)\n",
    "    intersection = keras.backend.sum(y_true_f * y_pred_f)\n",
    "    union = keras.backend.sum(y_true_f) + keras.backend.sum(y_pred_f) - intersection\n",
    "    return (intersection + smooth) / (union + smooth)\n",
    "\n",
    "\n",
    "## (2) Fonctions de perte\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    return 1 - dice_coefficient(y_true, y_pred)\n",
    "\n",
    "### les autres fonctions de perte seront définis à même le modèle \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ad58f-684e-4213-b4fd-252b62292506",
   "metadata": {},
   "source": [
    "## Partie 5 : Augmentation des données (Data Augmentation)\n",
    "\n",
    "On génère des données \"augmentées\", à savoir en faisant des modifications légères (retournement, rotation, zoom...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05dd6459-6d90-462b-9fa6-38dba96e8d70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T16:39:14.795867Z",
     "iopub.status.busy": "2025-03-02T16:39:14.793044Z",
     "iopub.status.idle": "2025-03-02T16:39:14.852088Z",
     "shell.execute_reply": "2025-03-02T16:39:14.848838Z",
     "shell.execute_reply.started": "2025-03-02T16:39:14.795794Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 17:39:14.815864: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.RandomFlip(\"horizontal\"),\n",
    "    keras.layers.RandomRotation(0.1),\n",
    "    keras.layers.RandomZoom(0.1)\n",
    "])\n",
    "\n",
    "def augmented_data_generator(images, masks, batch_size, img_height, img_width, num_classes):\n",
    "    for batch_images, batch_masks in data_generator(images, masks, batch_size, img_height, img_width, num_classes):\n",
    "        augmented_images = data_augmentation(batch_images)\n",
    "        yield augmented_images, batch_masks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3211eb-a2a7-4ebe-ae27-43b83bf7343a",
   "metadata": {},
   "source": [
    "## Partie 6 : Entraînement des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ad9ba-cbbf-47b3-aaba-59e5e29a7d03",
   "metadata": {},
   "source": [
    "### Configuration des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bbffd02-01a5-4b1a-a0eb-e42d3a894d29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T16:39:14.855668Z",
     "iopub.status.busy": "2025-03-02T16:39:14.854946Z",
     "iopub.status.idle": "2025-03-02T16:39:14.972112Z",
     "shell.execute_reply": "2025-03-02T16:39:14.971136Z",
     "shell.execute_reply.started": "2025-03-02T16:39:14.855619Z"
    }
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"UNet_mini\": build_unet_mini(IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES),\n",
    "    #\"UNet_base\": build_unet(IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES),\n",
    "    #\"VGG16_UNet\": build_vgg16_unet(IMG_HEIGHT, IMG_WIDTH, NUM_CLASSES),\n",
    "    #\"MobileNetV2_pretrained\": keras.applications.MobileNetV2(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), include_top=False, weights='imagenet')\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc128fd-5813-46c7-b797-5a41ddb91be9",
   "metadata": {},
   "source": [
    "### Configuration des loss (fonctions de perte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6151e77-e8a6-4408-adc7-aaf60be21c4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T16:39:14.974387Z",
     "iopub.status.busy": "2025-03-02T16:39:14.973512Z",
     "iopub.status.idle": "2025-03-02T16:39:14.979809Z",
     "shell.execute_reply": "2025-03-02T16:39:14.978568Z",
     "shell.execute_reply.started": "2025-03-02T16:39:14.974332Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "losses = {\n",
    "    \"categorical_crossentropy\": keras.losses.CategoricalCrossentropy(),\n",
    "    \"dice_loss\": dice_loss,\n",
    "    \"mixed_loss\": lambda y_true, y_pred: 0.5 * keras.losses.CategoricalCrossentropy()(y_true, y_pred) + 0.5 * dice_loss(y_true, y_pred)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0222a82-6832-4ecf-8be6-cb4660d21d28",
   "metadata": {},
   "source": [
    "### Entraînement des modèles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d740093e-f8a1-4df5-8503-155d82c7f8e6",
   "metadata": {},
   "source": [
    "Entraînement **sans** data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b17a526-9af1-4c83-8989-58670933a673",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T17:03:12.833723Z",
     "iopub.status.busy": "2025-03-02T17:03:12.832649Z",
     "iopub.status.idle": "2025-03-02T17:05:08.100569Z",
     "shell.execute_reply": "2025-03-02T17:05:08.099196Z",
     "shell.execute_reply.started": "2025-03-02T17:03:12.833649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting UNet_mini_categorical_crossentropy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 515ms/step - iou_metric: 0.3229 - loss: 1.1035"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 684ms/step - iou_metric: 0.3210 - loss: 1.1064 - val_iou_metric: 0.2477 - val_loss: 1.2507\n",
      "Epoch 2/3\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - iou_metric: 0.0000e+00 - loss: 0.0000e+00 - val_iou_metric: 0.2477 - val_loss: 1.2507\n",
      "Epoch 3/3\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437ms/step - iou_metric: 0.3360 - loss: 1.0290"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 804ms/step - iou_metric: 0.3357 - loss: 1.0301 - val_iou_metric: 0.2673 - val_loss: 1.2364\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 408ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/02 18:03:49 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpbjex21ab/model, flavor: tensorflow). Fall back to return ['tensorflow==2.18.0', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/03/02 18:03:49 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n",
      "2025/03/02 18:03:56 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp40gxbkq9/model, flavor: keras). Fall back to return ['keras==3.8.0']. Set logging level to DEBUG to see the full traceback. \n",
      "\u001b[31m2025/03/02 18:03:56 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'UNet_mini_categorical_crossentropy' already exists. Creating a new version of this model...\n",
      "Created version '5' of model 'UNet_mini_categorical_crossentropy'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting UNet_mini_dice_loss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 424ms/step - iou_metric: 0.3973 - loss: 0.4331"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 578ms/step - iou_metric: 0.3988 - loss: 0.4317 - val_iou_metric: 0.3576 - val_loss: 0.4792\n",
      "Epoch 2/3\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - iou_metric: 0.0000e+00 - loss: 0.0000e+00 - val_iou_metric: 0.3576 - val_loss: 0.4792\n",
      "Epoch 3/3\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462ms/step - iou_metric: 0.4614 - loss: 0.3703"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 586ms/step - iou_metric: 0.4607 - loss: 0.3710 - val_iou_metric: 0.3775 - val_loss: 0.4564\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 445ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/02 18:04:25 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp938j7_rw/model, flavor: tensorflow). Fall back to return ['tensorflow==2.18.0', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/03/02 18:04:26 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n",
      "2025/03/02 18:04:34 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpexmu5u6f/model, flavor: keras). Fall back to return ['keras==3.8.0']. Set logging level to DEBUG to see the full traceback. \n",
      "\u001b[31m2025/03/02 18:04:34 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'UNet_mini_dice_loss' already exists. Creating a new version of this model...\n",
      "Created version '5' of model 'UNet_mini_dice_loss'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting UNet_mini_mixed_loss\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 413ms/step - iou_metric: 0.4222 - loss: 1.7754"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 550ms/step - iou_metric: 0.4169 - loss: 1.7453 - val_iou_metric: 0.1910 - val_loss: 1.1406\n",
      "Epoch 2/3\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 100ms/step - iou_metric: 0.0000e+00 - loss: 0.0000e+00 - val_iou_metric: 0.1910 - val_loss: 1.1406\n",
      "Epoch 3/3\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 418ms/step - iou_metric: 0.3292 - loss: 0.7923"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 528ms/step - iou_metric: 0.3294 - loss: 0.7924 - val_iou_metric: 0.3175 - val_loss: 0.8951\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 381ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/02 18:05:00 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmptlrxartf/model, flavor: tensorflow). Fall back to return ['tensorflow==2.18.0', 'cloudpickle==3.1.1']. Set logging level to DEBUG to see the full traceback. \n",
      "2025/03/02 18:05:00 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n",
      "2025/03/02 18:05:08 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpuppn1_us/model, flavor: keras). Fall back to return ['keras==3.8.0']. Set logging level to DEBUG to see the full traceback. \n",
      "\u001b[31m2025/03/02 18:05:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'UNet_mini_mixed_loss' already exists. Creating a new version of this model...\n",
      "Created version '5' of model 'UNet_mini_mixed_loss'.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "os.environ['KERAS_SERIALIZATION_FORMAT'] = 'tf'  # Ou 'keras'\n",
    "\n",
    "# Calculer steps_per_epoch\n",
    "steps_per_epoch = math.ceil(len(train_images) / BATCH_SIZE)\n",
    "\n",
    "# Calculer validation_steps\n",
    "validation_steps = math.ceil(len(val_images) / BATCH_SIZE)\n",
    "\n",
    "\n",
    "## On itère sur les modèles\n",
    "for model_name, model in models.items():\n",
    "    ## On itère (2e boucle) sur les loss\n",
    "    for loss_name, loss in losses.items():\n",
    "        ## on sauve les résultats dans MFlow (model + loss)\n",
    "        with mlflow.start_run(run_name=f\"{model_name}_{loss_name}\"):\n",
    "\n",
    "            \n",
    "            ## génération des données d'entraînement\n",
    "            train_generator = data_generator(train_images,\n",
    "                                 train_masks,\n",
    "                                 BATCH_SIZE,\n",
    "                                 NUM_CLASSES)\n",
    "\n",
    "            ## génération des données de validation\n",
    "            val_generator = data_generator(val_images,\n",
    "                               val_masks,\n",
    "                               BATCH_SIZE,\n",
    "                               NUM_CLASSES)\n",
    "\n",
    "            ## fine-tuning du modèle préentraîné \n",
    "            if \"pretrained\" in model_name:\n",
    "                x = model.output\n",
    "                \n",
    "                ## ajout d'un décodeur pour la segmentation\n",
    "                x = keras.layers.Conv2DTranspose(256, 3, strides=2, padding='same')(x)\n",
    "                x = keras.layers.Conv2DTranspose(128, 3, strides=2, padding='same')(x)\n",
    "                outputs = keras.layers.Conv2D(NUM_CLASSES, 1, activation='softmax')(x)\n",
    "                model = keras.Model(inputs=model.input, outputs=outputs)\n",
    "            model.compile(optimizer='adam', loss=loss, metrics=[iou_metric])\n",
    "\n",
    "            print(f\"fitting {model_name}_{loss_name}\")\n",
    "\n",
    "            \n",
    "            model.fit(train_generator,\n",
    "                      validation_data=val_generator,\n",
    "                      epochs=EPOCHS,\n",
    "                     steps_per_epoch=steps_per_epoch,\n",
    "                     validation_steps = validation_steps)\n",
    "\n",
    "            # log model\n",
    "\n",
    "            mlflow.keras.log_model(model,\n",
    "                                   \"model\",\n",
    "                                   registered_model_name=f\"{model_name}_{loss_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c40eb-1b9d-42aa-a9bc-54d994b18f29",
   "metadata": {},
   "source": [
    "Entraînement **avec** data augmentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76f700eb-2779-425d-bbae-c8ffb9debf3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-02T16:39:14.981682Z",
     "iopub.status.busy": "2025-03-02T16:39:14.981161Z",
     "iopub.status.idle": "2025-03-02T16:40:53.203197Z",
     "shell.execute_reply": "2025-03-02T16:40:53.201378Z",
     "shell.execute_reply.started": "2025-03-02T16:39:14.981632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting UNet_mini_categorical_crossentropy\n",
      "Epoch 1/3\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 601ms/step - iou_metric: 0.0811 - loss: 1.9751 - val_iou_metric: 0.1425 - val_loss: 2.0858\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 17:39:26.421126: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/home/mehdi/Documents/OC/OC8/.venv/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 186ms/step - iou_metric: 0.0000e+00 - loss: 0.0000e+00 - val_iou_metric: 0.1425 - val_loss: 2.0858\n",
      "Epoch 3/3\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 544ms/step - iou_metric: 0.1287 - loss: 1.7428 - val_iou_metric: 0.1174 - val_loss: 1.7757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/02 17:39:49 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n",
      "2025/03/02 17:39:57 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmp1bz50m9i/model, flavor: keras). Fall back to return ['keras==3.8.0']. Set logging level to DEBUG to see the full traceback. \n",
      "\u001b[31m2025/03/02 17:39:57 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'UNet_mini_categorical_crossentropy' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'UNet_mini_categorical_crossentropy'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting UNet_mini_dice_loss\n",
      "Epoch 1/3\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 528ms/step - iou_metric: 0.1621 - loss: 0.7240 - val_iou_metric: 0.2543 - val_loss: 0.5959\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 17:40:07.302034: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 102ms/step - iou_metric: 0.0000e+00 - loss: 0.0000e+00 - val_iou_metric: 0.2543 - val_loss: 0.5959\n",
      "Epoch 3/3\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 506ms/step - iou_metric: 0.2449 - loss: 0.6084 - val_iou_metric: 0.2544 - val_loss: 0.5958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/02 17:40:18 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n",
      "2025/03/02 17:40:25 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpnu3qkd_3/model, flavor: keras). Fall back to return ['keras==3.8.0']. Set logging level to DEBUG to see the full traceback. \n",
      "\u001b[31m2025/03/02 17:40:25 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting UNet_mini_mixed_loss\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'UNet_mini_dice_loss' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'UNet_mini_dice_loss'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 608ms/step - iou_metric: 0.2329 - loss: 5.8786 - val_iou_metric: 0.1035 - val_loss: 1.3726\n",
      "Epoch 2/3\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 111ms/step - iou_metric: 0.0000e+00 - loss: 0.0000e+00 - val_iou_metric: 0.1035 - val_loss: 1.3726\n",
      "Epoch 3/3\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 591ms/step - iou_metric: 0.0892 - loss: 1.3656 - val_iou_metric: 0.0837 - val_loss: 1.3807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/02 17:40:45 WARNING mlflow.keras.save: You are saving a Keras model without specifying model signature.\n",
      "/home/mehdi/Documents/OC/OC8/.venv/lib/python3.10/site-packages/keras/src/saving/serialization_lib.py:390: UserWarning: The object being serialized includes a `lambda`. This is unsafe. In order to reload the object, you will have to pass `safe_mode=False` to the loading function. Please avoid using `lambda` in the future, and use named Python functions instead. This is the `lambda` being serialized:     \"mixed_loss\": lambda y_true, y_pred: 0.5 * keras.losses.CategoricalCrossentropy()(y_true, y_pred) + 0.5 * dice_loss(y_true, y_pred)\n",
      "\n",
      "  return {key: serialize_keras_object(value) for key, value in obj.items()}\n",
      "2025/03/02 17:40:51 WARNING mlflow.utils.environment: Encountered an unexpected error while inferring pip requirements (model URI: /tmp/tmpy1ld2lew/model, flavor: keras). Fall back to return ['keras==3.8.0']. Set logging level to DEBUG to see the full traceback. \n",
      "\u001b[31m2025/03/02 17:40:51 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
      "Registered model 'UNet_mini_mixed_loss' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'UNet_mini_mixed_loss'.\n",
      "2025/03/02 17:40:51 WARNING mlflow.tensorflow: Encountered unexpected error while inferring batch size from training dataset: data_generator() takes 4 positional arguments but 6 were given\n",
      "2025/03/02 17:40:51 WARNING mlflow.tensorflow: Unrecognized dataset type <class 'generator'>. Dataset logging skipped.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting UNet_mini_categorical_crossentropy_augmented\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 17:40:52.073260: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 91\u001b[0m\n\u001b[1;32m     88\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mtensorflow\u001b[38;5;241m.\u001b[39mautolog()\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfitting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_augmented\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 91\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43maugmented_train_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m         \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m         \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# log model\u001b[39;00m\n\u001b[1;32m     98\u001b[0m mlflow\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlog_model(model,\n\u001b[1;32m     99\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    100\u001b[0m                        registered_model_name\u001b[38;5;241m=\u001b[39mmodel_name)\n",
      "File \u001b[0;32m~/Documents/OC/OC8/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:590\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    580\u001b[0m try_log_autologging_event(\n\u001b[1;32m    581\u001b[0m     AutologgingEventLogger\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39mlog_patch_function_start,\n\u001b[1;32m    582\u001b[0m     session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    586\u001b[0m     kwargs,\n\u001b[1;32m    587\u001b[0m )\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m patch_is_class:\n\u001b[0;32m--> 590\u001b[0m     \u001b[43mpatch_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_original\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    592\u001b[0m     patch_function(call_original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/OC/OC8/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:166\u001b[0m, in \u001b[0;36mPatchFunction.call\u001b[0;34m(cls, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mcls\u001b[39m, original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/OC/OC8/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:177\u001b[0m, in \u001b[0;36mPatchFunction.__call__\u001b[0;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_exception(e)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# Regardless of what happens during the `_on_exception` callback, reraise\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# the original implementation exception once the callback completes\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m~/Documents/OC/OC8/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:170\u001b[0m, in \u001b[0;36mPatchFunction.__call__\u001b[0;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 170\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_patch_implementation\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    172\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/OC/OC8/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:233\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.PatchWithManagedRun._patch_implementation\u001b[0;34m(self, original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mactive_run() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _has_active_training_session():\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanaged_run \u001b[38;5;241m=\u001b[39m create_managed_run()\n\u001b[0;32m--> 233\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_patch_implementation\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanaged_run:\n\u001b[1;32m    236\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mend_run(RunStatus\u001b[38;5;241m.\u001b[39mto_string(RunStatus\u001b[38;5;241m.\u001b[39mFINISHED))\n",
      "File \u001b[0;32m~/Documents/OC/OC8/.venv/lib/python3.10/site-packages/mlflow/tensorflow/__init__.py:1353\u001b[0m, in \u001b[0;36mautolog.<locals>.FitPatch._patch_implementation\u001b[0;34m(self, original, inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1347\u001b[0m         _logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1348\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to log training dataset information to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1349\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLflow Tracking. Reason: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1350\u001b[0m             e,\n\u001b[1;32m   1351\u001b[0m         )\n\u001b[0;32m-> 1353\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43moriginal\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_models:\n\u001b[1;32m   1356\u001b[0m     _log_keras_model(history, args)\n",
      "File \u001b[0;32m~/Documents/OC/OC8/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:573\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original\u001b[0;34m(*og_args, **og_kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m         original_result \u001b[38;5;241m=\u001b[39m original(\u001b[38;5;241m*\u001b[39m_og_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_og_kwargs)\n\u001b[1;32m    571\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n\u001b[0;32m--> 573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_original_fn_with_event_logging\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_original_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mog_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/OC/OC8/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:508\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original_fn_with_event_logging\u001b[0;34m(original_fn, og_args, og_kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    501\u001b[0m         AutologgingEventLogger\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39mlog_original_function_start,\n\u001b[1;32m    502\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    506\u001b[0m         og_kwargs,\n\u001b[1;32m    507\u001b[0m     )\n\u001b[0;32m--> 508\u001b[0m     original_fn_result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mog_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mog_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m     try_log_autologging_event(\n\u001b[1;32m    511\u001b[0m         AutologgingEventLogger\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39mlog_original_function_success,\n\u001b[1;32m    512\u001b[0m         session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    516\u001b[0m         og_kwargs,\n\u001b[1;32m    517\u001b[0m     )\n\u001b[1;32m    518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_fn_result\n",
      "File \u001b[0;32m~/Documents/OC/OC8/.venv/lib/python3.10/site-packages/mlflow/utils/autologging_utils/safety.py:570\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function.<locals>.call_original.<locals>._original_fn\u001b[0;34m(*_og_args, **_og_kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[38;5;66;03m# Show all non-MLflow warnings as normal (i.e. not as event logs)\u001b[39;00m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;66;03m# during original function execution, even if silent mode is enabled\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# (`silent=True`), since these warnings originate from the ML framework\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# or one of its dependencies and are likely relevant to the caller\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_non_mlflow_warnings_behavior_for_current_thread(\n\u001b[1;32m    567\u001b[0m     disable_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    568\u001b[0m     reroute_warnings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    569\u001b[0m ):\n\u001b[0;32m--> 570\u001b[0m     original_result \u001b[38;5;241m=\u001b[39m \u001b[43moriginal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_og_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_og_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m original_result\n",
      "File \u001b[0;32m~/Documents/OC/OC8/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Documents/OC/OC8/.venv/lib/python3.10/site-packages/keras/src/trainers/data_adapters/generator_data_adapter.py:16\u001b[0m, in \u001b[0;36mGeneratorDataAdapter.__init__\u001b[0;34m(self, generator)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_batches \u001b[38;5;241m=\u001b[39m first_batches\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[43mfirst_batches\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen passing a Python generator to a Keras model, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe generator must return a tuple, either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfirst_batches[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m     )\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Entraînement avec augmentation de données\n",
    "for model_name, model in models.items():\n",
    "    for loss_name, loss in losses.items():\n",
    "        with mlflow.start_run(run_name=f\"{model_name}_{loss_name}_augmented\"):\n",
    "\n",
    "            ## génération des données d'entraînement augmentées\n",
    "            augmented_train_generator = augmented_data_generator(train_images,\n",
    "                                                     train_masks,\n",
    "                                                     BATCH_SIZE,\n",
    "                                                     IMG_HEIGHT,\n",
    "                                                     IMG_WIDTH,\n",
    "                                                     NUM_CLASSES)\n",
    "            ## génération des données de validation\n",
    "            val_generator = data_generator(val_images,\n",
    "                               val_masks,\n",
    "                               BATCH_SIZE,\n",
    "                               NUM_CLASSES)\n",
    "\n",
    "            \n",
    "            if \"pretrained\" in model_name:\n",
    "                \n",
    "                ## ajout d'un décodeur pour la segmentation\n",
    "                x = model.output\n",
    "                x = keras.layers.Conv2DTranspose(256, 3, strides=2, padding='same')(x)\n",
    "                x = keras.layers.Conv2DTranspose(128, 3, strides=2, padding='same')(x)\n",
    "                outputs = keras.layers.Conv2D(NUM_CLASSES, 1, activation='softmax')(x)\n",
    "                model = keras.Model(inputs=model.input, outputs=outputs)\n",
    "            model.compile(optimizer='adam', loss=loss, metrics=[iou_metric])\n",
    "            mlflow.tensorflow.autolog()\n",
    "\n",
    "            print(f\"fitting {model_name}_{loss_name}_augmented\")\n",
    "            \n",
    "            model.fit(augmented_train_generator,\n",
    "                      validation_data=val_generator,\n",
    "                      epochs=EPOCHS)\n",
    "\n",
    "            # log model\n",
    "            mlflow.keras.log_model(model,\n",
    "                                   \"model\",\n",
    "                                   registered_model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f247ce3-d6e6-43fe-bae0-d1b7732a4a2c",
   "metadata": {},
   "source": [
    "## Partie 7 : Évaluation des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f05fd-acff-40ca-832b-afe8bd4e0d6b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-02T16:40:53.204687Z",
     "iopub.status.idle": "2025-03-02T16:40:53.206024Z",
     "shell.execute_reply": "2025-03-02T16:40:53.205595Z",
     "shell.execute_reply.started": "2025-03-02T16:40:53.205551Z"
    }
   },
   "outputs": [],
   "source": [
    "# Évaluation des modèles entraînés\n",
    "for model_name, model in models.items():\n",
    "    for loss_name, loss in losses.items():\n",
    "\n",
    "\n",
    "        ## génération des données de test\n",
    "        test_generator = data_generator(test_images,test_masks,\n",
    "                                                     BATCH_SIZE,\n",
    "                                                     IMG_HEIGHT,\n",
    "                                                     IMG_WIDTH,\n",
    "                                                     NUM_CLASSES)\n",
    "        # Chargement du modèle entraîné\n",
    "        model_path = f\"mlruns/0/{mlflow.search_runs(filter_string=f'tags.mlflow.runName = \\'{model_name}_{loss_name}\\'').iloc[0].run_id}/artifacts/model/data/model.keras\"\n",
    "        loaded_model = keras.models.load_model(model_path, custom_objects={'iou_metric': iou_metric, 'dice_loss': dice_loss})\n",
    "\n",
    "        # Évaluation du modèle sur l'ensemble de test\n",
    "        loss, iou = loaded_model.evaluate(test_generator)\n",
    "        print(f\"Modèle {model_name} avec perte {loss_name} : Loss = {loss}, IoU = {iou}\")\n",
    "\n",
    "# Évaluation des modèles entraînés avec augmentation de données\n",
    "for model_name, model in models.items():\n",
    "    for loss_name, loss in losses.items():\n",
    "\n",
    "        ## génération des données de test\n",
    "        test_generator = data_generator(test_images,test_masks,\n",
    "                                                     BATCH_SIZE,\n",
    "                                                     IMG_HEIGHT,\n",
    "                                                     IMG_WIDTH,\n",
    "                                                     NUM_CLASSES)\n",
    "        \n",
    "        # Chargement du modèle entraîné\n",
    "        model_path = f\"mlruns/0/{mlflow.search_runs(filter_string=f'tags.mlflow.runName = \\'{model_name}_{loss_name}_augmented\\'').iloc[0].run_id}/artifacts/model/data/model.keras\"\n",
    "        loaded_model = keras.models.load_model(model_path, custom_objects={'iou_metric': iou_metric, 'dice_loss': dice_loss})\n",
    "\n",
    "        # Évaluation du modèle sur l'ensemble de test\n",
    "        loss, iou = loaded_model.evaluate(test_generator)\n",
    "        print(f\"Modèle {model_name} avec perte {loss_name} et augmentation : Loss = {loss}, IoU = {iou}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ba8d2c-b924-434f-83bb-db064550ae33",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce notebook a exploré différentes approches pour la segmentation d'images de scènes de rue. Les résultats montrent que l'augmentation de données peut améliorer les performances des modèles. \n",
    "\n",
    "### Améliorations possibles\n",
    "\n",
    "* Tester d'autres modèles de segmentation, tels que SegNet ou DeepLabv3.\n",
    "* Expérimenter avec différentes fonctions de perte et métriques.\n",
    "* Utiliser des techniques d'augmentation de données plus avancées.\n",
    "* Optimiser les hyperparamètres des modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13142c69-6342-4a17-93c3-182b596cf61b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
